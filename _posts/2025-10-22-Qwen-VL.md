---
title: "Qwen-VL"
author: curya
date: 2025-10-21
categories: [Blogs,BasicKnow,LLM]
tags: [BasicKnow]
math: true
mermaid: true
---

论文标题：Qwen2.5-VL Technical Report

**核心**
- 模型能力：
    - 更强的视觉识别、精准的目标定位、稳健的文档解析、长视频理解（能够使用「边界框/点」准确定位目标，可以对发票、表单和表格等结构化数据进行稳健信息抽取，对图表、示意图、布局等进行详细分析）
    - 交互式视觉智能体：具备推理、工具使用和任务执行能力，能够胜任如操作电脑、移动设备等真实场景
- **动态分辨率处理** & **绝对时间编码**：可以处理不同分辨率的图像和时长达数小时的视频，具备秒级事件定位能力（*赋予模型原生的空间尺寸和时间动态感知能力，无需依赖传统归一化技术*）。
    - 从零训练原生动态分辨率的ViT（native dynamic-resolution Vision Transformer），并引入了窗口注意力机制，在保留原生分辨率的同时大幅降低计算开销

## 1. Introduction
主流范式：视觉编码器、跨模态投影层、LLM

面临问题：计算复杂度高、上下文理解有限、细粒度视觉感知能力弱以及在不同序列长度下性能不稳定等问题。

Qwen2.5-VL：细粒度感知能力，具体贡献如下
1. 在视觉编码器中实现了**窗口注意力机制**，以优化推理效率；
2. 引入**动态帧率采样**，将动态分辨率扩展到**时间维度**，实现对不同采样率视频的全面理解；
3. 在**时间域升级了 MRoPE**，通过对齐绝对时间，促进了更高效的时间序列学习；
4. 在**高质量数据**整理方面做出了重大努力，无论是预训练还是有监督微调，并将预训练语料库规模从 1.2T 扩展到 4.1T tokens。

Qwen2.5-VL的突出特性包括：
- 强大的文档解析能力：Qwen2.5-VL将文本识别升级为全能文档解析，在多场景、多语言及多种内嵌（如手写、表格、图表、化学公式、乐谱）文档处理方面表现卓越。
- 跨格式的精准目标定位：Qwen2.5-VL在目标检测、指认和计数方面显著提升了准确性，支持**绝对坐标**和JSON格式，实现高级空间推理。
- 超长视频理解与细粒度视频定位：模型将原生动态分辨率扩展到时间维度，能够理解时长达数小时的视频，并在秒级范围内提取事件片段。
- 增强的终端智能体能力：模型具备高级的定位、推理和决策能力，极大提升了其在手机和电脑等终端设备上的智能体功能。

## 2. 方法
视觉编码器：在结构上，融入了FFN with SwiGLU、RMSNorm、窗口注意力 --> 提升性能 & 效率
- 原生分辨率处理、动态帧率采样：不同尺寸的图像和不同帧率的视频会映射为不同长度的token序列
- MRoPE：多模态位置编码，能够在事件维度将时间ID与绝对时间对齐，使模型能够更好理解时间动态（如事件节奏、精确时刻的定位）。
<img width="2938" height="2560" alt="image" src="https://github.com/user-attachments/assets/b000a72e-ef3c-4a9a-8195-bb1dc66ba0df" />

### 2.1 模型结构
Qwen2.5-VL模型包含3个模块：
- LLM：模型初始化自Qwen2.5的预训练权重。1D RoPE --> MRoPE，实现与绝对时间对齐。
- Vision Encoder：重新设计的ViT架构。结合2D-RoPE和窗口注意力，支持原生分辨率&加速视觉编码器计算效率。训练&推理过程中，输入图像的宽高会调整为28的倍数输入，patch切分的步幅是14。
- MLP-based Vision-Language Merger：MLP映射，图像特征压缩&维度对齐。空间相邻4个patch特征分组、拼接，MLP进行映射，投影到LLM维度。

不同尺寸的Qwen2.5-VL的结构如下（视觉编码器尺寸都是一样的）：
<img width="2504" height="1816" alt="image" src="https://github.com/user-attachments/assets/04726a4c-03ac-4fa8-bdee-516d7882d8e6" />

#### 2.1.1 视觉编码器
**问题**：处理原生分辨率（有的大有的小），导致训练/推理计算负载不均衡。

**优化**：
1. 引入窗口注意力（4个层使用全局注意力，其余层使用窗口注意力，尺寸为112x112）
2. 采用2D RoPE，捕获二维空间中的位置关系
3. 视频处理：扩展到三维patch分割，14x14作为patch基本单元，对视频采用两个连续视频帧作为一组
4. 模型结构：RMSNorm、SwiGLU，提升视觉和语言之间的计算效率和兼容性 --> 和语言模型的结构设计对齐，保持一致
5. 训练：从头训练，CLIP预训练、视觉-语言对齐、端到端微调多个阶段；原生分辨率动态采样，按照图片的原始长宽比进行随机采样，从而提升模型对不同分辨率输入的泛化能力

#### 2.1.2 原生动态分辨率&帧率
空间域：Qwen2.5-VL能够动态地将不同尺寸的图像转换为对应长度的token序列。
- 使用输入图像的实际尺寸（**绝对坐标**）表示边界框、点及其他空间特征，使模型能够原生学习尺度信息，提升其处理不同分辨率图像的能力。（**但是在Qwen3-VL又换回了相对坐标**）

视频输入：Qwen2.5-VL引入了**动态帧率**（FPS）训练和**绝对时间编码**。
- 通过适应不同帧率，模型能够更好地捕捉视频内容的时间动态。
- 不同于其他方法通过增加文本时间戳或引入额外头部实现时间定位，Qwen2.5-VL提出将MRoPE的ID直接与时间戳对齐。该方案允许模型通过时间维度ID之间的间隔理解时间节奏，无需任何额外的计算开销。

#### 2.1.3 MRoPE和绝对时间对齐
MRoPE：在Qwen2-VL中提出，将位置嵌入分解为三个分量（时间、高度和宽度）
- 文本输入，三个分量使用相同的位置ID，等价于1D RoPE
- 图像输入，视觉token的时间ID保持不变，高度和宽度分量根据token在图像中的空间位置分配不同的ID
- 视频输入，视为帧序列，时间ID随每一帧递增，高度和宽度分量与静态图像保持一致

问题：Qwen2-VL中MRoPE的时间位置ID仅与输入帧数量相关，不能反映内容变化的速度或视频中事件的绝对时间。

改进：Qwen2.5-VL中，将MRoPE的时间分量对齐到绝对时间。模型通过利用时间ID之间的间隔，可以在不同FPS采样的视频之间实现一致的时间对齐学习。

### 2.2 预训练
#### 2.2.1 预训练数据
<img width="2640" height="902" alt="image" src="https://github.com/user-attachments/assets/e45d7557-71f9-42db-836f-af3b7fb8da70" />

- 数据规模：1.2T tokens (Qwen2-VL) --> **4T tokens (Qwen2.5-VL)**
- 数据类型：图像描述、图文混合数据、OCR数据、视觉知识（如名人、地标、动植物识别）、多模态学术问答、定位数据、文档解析数据、视频描述、视频定位以及基于智能体的交互数据
- 数据来源：清洗原始网页数据、合成数据等

**图文混合数据**  
图文混合数据对于多模态学习至关重要，主要带来三大益处：
（1）使模型能借助视觉与文本线索实现上下文学习（Alayrac et al., 2022）；
（2）在图片缺失时，保持强大的纯文本能力（Lin et al., 2024）；
（3）包含大量通用信息。然而，目前可用的图文混合数据大多缺乏有意义的图文关联，且噪声较多，限制了其在复杂推理和创新生成上的作用。

为了应对这些挑战，我们设计了数据打分和清洗流程，确保只使用高质量且相关性的图文混合数据。流程分两步：先进行标准数据清洗，再利用内部评测模型进行四阶段打分。打分标准包括：
（1）文本质量、
（2）图文相关性、
（3）图文互补性和
（4）信息密度平衡。
这样的精细流程显著提升了模型进行复杂推理和生成连贯多模态内容的能力。

下面是具体打分标准描述：  
- 图文相关性：分数越高，图像与文本间关联越强，图像对于文本能有效补充、解释或扩展，而非仅作装饰。
- 信息互补性：分数越高，图像和文本互为补充，两者各自提供独特细节，共同构成完整语境。
- 信息密度平衡：分数越高，图像与文本的信息分布越均衡，避免单一维度信息过多，保证两者协同。

**绝对坐标定位数据**  
采用原生分辨率训练，以更准确地感知世界。与只使用相对坐标不同，绝对坐标能更有效地表达物体在图片中的实际大小与位置。因此，在训练时Qwen2.5-VL基于输入图片的实际尺寸来表示边框与关键点，从而帮助模型更好地把握真实世界中的尺度和空间关联，提升目标检测与定位等任务的表现。

为提升模型在定位上的泛化能力，我们联合公开数据集和自有数据，构建了覆盖边框、关键点和指代表达的综合数据集。我们采用多种格式（如XML、JSON和自定义）进行合成，利用复制粘贴增强及现成模型进行合成，从而实现更全面的能力评测和进步。

针对开放词汇目标检测，我们将训练数据类别扩展至10000+对象。此外，为提升模型在极端目标检测场景下的表现，我们在查询中合成了不存在的目标类别，并为每个目标类别创建含多个实例的图片数据。

为确保模型具备优异的点目标定位能力，我们构建了涵盖公开及合成数据的点定位数据集。包含PixMo公开的点标记和计数、公开的目标定位（检测和实例分割），以及通过自动化流程生成的针对图片细节精准指点数据。

**文档全能解析数据**  
针对Qwen2.5-VL训练，我们合成了大量文档数据。传统文档解析多用分模型分别处理版面分析、文本提取、图表理解、插图识别等任务。而Qwen2.5-VL则致力于打造通用模型，实现文档结构的解析、理解和格式转化。我们在文档中融入多种元素，包括表格、图表、公式、自然或合成图片、乐谱及化学式，并全部统一为HTML格式，借助标签结构描述布局框和插图信息。同时，依据实际阅读顺序丰富了文档布局，为每个结构（如段落、图表）在HTML基准中加入坐标信息。这一创新让所有文档信息（包括结构、文本、图表和插图）都能以标准化且统一的方式表示，从而实现多模态文档元素的无缝融合，极大提升文档理解和转换效率与准确性。

**OCR数据**  
为提升OCR性能，我们整合并整理了多种来源的数据，包括合成数据、开源数据和自采数据。合成数据由视觉文本生成引擎自动生成，覆盖多样的真实场景文本图片。为支持更多语言，提升多语种能力，我们引入了大规模多语种OCR数据集，其中涵盖法语、德语、意大利语、西班牙语、葡萄牙语、阿拉伯语、俄语、日语、韩语和越南语等多种语言。该数据集既包含高质量合成图片，也包含真实场景自然图片，保证多文本样式和环境的丰富性，有助于模型在不同语境下展现强适应力。针对图表数据，我们利用matplotlib、seaborn、plotly等可视化库合成了100万样本，涵盖柱状图、关系图、热力图等类型。针对表格数据，我们基于离线端到端表格识别模型处理了600万真实样本，过滤出置信度低、重叠及单元格密度不足的表格。

**视频数据**  
为提升模型对不同帧率（FPS）的视频理解能力，我们在训练阶段动态采样FPS，从而实现训练样本中FPS的均匀分布。对于超过半小时的视频，专门通过合成流程生成了多帧字幕。针对视频锚定，采用了秒级和时分秒帧（hmsf）格式的时间戳，确保模型能准确理解和输出不同时间格式的结果。

**智能体数据**  
我们通过数据采集和合成增强Qwen2.5-VL的感知和决策能力，赋予智能体能力。在感知部分，收集了移动端、网页、桌面端的截图，并使用合成引擎生成截图描述和界面元素定位标注。图片描述任务帮助模型理解图形界面，定位任务帮助其对齐元素外观与功能。在决策部分，我们将手机、网页和桌面端的操作统一为具备共享动作空间的函数调用格式，并基于开源数据及智能体框架（Wang等, 2025; 2024b;c）在虚拟环境下合成收集多步轨迹，统一重构为函数形式。每一步，由人工和模型共同注释生成推理过程（Xu等, 2024）：在给定真实操作后，我们在截图上高亮该操作，同时向注释者提供全局任务、操作前后截图，并要求书写推理内容以解释操作意图。此外，我们引入模型过滤筛除低质量推理，保证高质量数据。这些推理内容可防止Qwen2.5-VL仅拟合真实操作，使其在实际场景更具鲁棒性与泛化能力。

#### 2.2.2 训练

### 2.3 后训练
#### 2.3.1 指令数据

#### 2.3.2 数据过滤流水线

#### 2.3.3 拒绝采样增强推理

#### 2.3.4 训练
