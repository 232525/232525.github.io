---
title: "Qwen-VL"
author: curya
date: 2025-10-21
categories: [Blogs,BasicKnow,LLM]
tags: [BasicKnow]
math: true
mermaid: true
---

论文标题：Qwen2.5-VL Technical Report

**核心**
- 模型能力：
    - 更强的视觉识别、精准的目标定位、稳健的文档解析、长视频理解（能够使用「边界框/点」准确定位目标，可以对发票、表单和表格等结构化数据进行稳健信息抽取，对图表、示意图、布局等进行详细分析）
    - 交互式视觉智能体：具备推理、工具使用和任务执行能力，能够胜任如操作电脑、移动设备等真实场景
- **动态分辨率处理** & **绝对时间编码**：可以处理不同分辨率的图像和时长达数小时的视频，具备秒级事件定位能力（*赋予模型原生的空间尺寸和时间动态感知能力，无需依赖传统归一化技术*）。
    - 从零训练原生动态分辨率的ViT（native dynamic-resolution Vision Transformer），并引入了窗口注意力机制，在保留原生分辨率的同时大幅降低计算开销

## 1. Introduction
主流范式：视觉编码器、跨模态投影层、LLM

面临问题：计算复杂度高、上下文理解有限、细粒度视觉感知能力弱以及在不同序列长度下性能不稳定等问题。

Qwen2.5-VL：细粒度感知能力，具体贡献如下
1. 在视觉编码器中实现了**窗口注意力机制**，以优化推理效率；
2. 引入**动态帧率采样**，将动态分辨率扩展到**时间维度**，实现对不同采样率视频的全面理解；
3. 在**时间域升级了 MRoPE**，通过对齐绝对时间，促进了更高效的时间序列学习；
4. 在**高质量数据**整理方面做出了重大努力，无论是预训练还是有监督微调，并将预训练语料库规模从 1.2T 扩展到 4.1T tokens。

## 2. 方法
视觉编码器：在结构上，融入了FFN with SwiGLU、RMSNorm、窗口注意力 --> 提升性能 & 效率
- 原生分辨率处理、动态帧率采样：不同尺寸的图像和不同帧率的视频会映射为不同长度的token序列
- MRoPE：多模态位置编码，能够在事件维度将时间ID与绝对时间对齐，使模型能够更好理解时间动态（如事件节奏、精确时刻的定位）。
<img width="2938" height="2560" alt="image" src="https://github.com/user-attachments/assets/b000a72e-ef3c-4a9a-8195-bb1dc66ba0df" />

### 2.1 模型结构
Qwen2.5-VL模型包含3个模块：
- LLM：模型初始化自Qwen2.5的预训练权重。1D RoPE --> MRoPE，实现与绝对时间对齐。
- Vision Encoder：重新设计的ViT架构。结合2D-RoPE和窗口注意力，支持原生分辨率&加速视觉编码器计算效率。训练&推理过程中，输入图像的宽高会调整为28的倍数输入，patch切分的步幅是14。
- MLP-based Vision-Language Merger：MLP映射，图像特征压缩&维度对齐。空间相邻4个patch特征分组、拼接，MLP进行映射，投影到LLM维度。
