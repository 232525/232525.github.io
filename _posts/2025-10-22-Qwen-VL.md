---
title: "Qwen-VL"
author: curya
date: 2025-10-21
categories: [Blogs,BasicKnow,LLM]
tags: [BasicKnow]
math: true
mermaid: true
---

论文标题：Qwen2.5-VL Technical Report

**核心**
- 模型能力：
    - 更强的视觉识别、精准的目标定位、稳健的文档解析、长视频理解（能够使用「边界框/点」准确定位目标，可以对发票、表单和表格等结构化数据进行稳健信息抽取，对图表、示意图、布局等进行详细分析）
    - 交互式视觉智能体：具备推理、工具使用和任务执行能力，能够胜任如操作电脑、移动设备等真实场景
- **动态分辨率处理** & **绝对时间编码**：可以处理不同分辨率的图像和时长达数小时的视频，具备秒级事件定位能力（*赋予模型原生的空间尺寸和时间动态感知能力，无需依赖传统归一化技术*）。
    - 从零训练原生动态分辨率的ViT（native dynamic-resolution Vision Transformer），并引入了窗口注意力机制，在保留原生分辨率的同时大幅降低计算开销

## 1. Introduction
主流范式：视觉编码器、跨模态投影层、LLM

面临问题：计算复杂度高、上下文理解有限、细粒度视觉感知能力弱以及在不同序列长度下性能不稳定等问题。

Qwen2.5-VL：细粒度感知能力，具体贡献如下
1. 在视觉编码器中实现了**窗口注意力机制**，以优化推理效率；
2. 引入**动态帧率采样**，将动态分辨率扩展到**时间维度**，实现对不同采样率视频的全面理解；
3. 在**时间域升级了 MRoPE**，通过对齐绝对时间，促进了更高效的时间序列学习；
4. 在**高质量数据**整理方面做出了重大努力，无论是预训练还是有监督微调，并将预训练语料库规模从 1.2T 扩展到 4.1T tokens。

## 2. 方法
视觉编码器：在结构上，融入了FFN with SwiGLU、RMSNorm、窗口注意力 --> 提升性能 & 效率
- 原生分辨率处理、动态帧率采样：不同尺寸的图像和不同帧率的视频会映射为不同长度的token序列
- MRoPE：多模态位置编码，能够在事件维度将时间ID与绝对时间对齐，使模型能够更好理解时间动态（如事件节奏、精确时刻的定位）。
<img width="2938" height="2560" alt="image" src="https://github.com/user-attachments/assets/b000a72e-ef3c-4a9a-8195-bb1dc66ba0df" />

### 2.1 模型结构
Qwen2.5-VL模型包含3个模块：
- LLM：模型初始化自Qwen2.5的预训练权重。1D RoPE --> MRoPE，实现与绝对时间对齐。
- Vision Encoder：重新设计的ViT架构。结合2D-RoPE和窗口注意力，支持原生分辨率&加速视觉编码器计算效率。训练&推理过程中，输入图像的宽高会调整为28的倍数输入，patch切分的步幅是14。
- MLP-based Vision-Language Merger：MLP映射，图像特征压缩&维度对齐。空间相邻4个patch特征分组、拼接，MLP进行映射，投影到LLM维度。

不同尺寸的Qwen2.5-VL的结构如下（视觉编码器尺寸都是一样的）：
<img width="2504" height="1816" alt="image" src="https://github.com/user-attachments/assets/04726a4c-03ac-4fa8-bdee-516d7882d8e6" />

#### 2.1.1 视觉编码器
**问题**：处理原生分辨率（有的大有的小），导致训练/推理计算负载不均衡。

**优化**：
1. 引入窗口注意力（4个层使用全局注意力，其余层使用窗口注意力，尺寸为112x112）
2. 采用2D RoPE，捕获二维空间中的位置关系
3. 视频处理：扩展到三维patch分割，14x14作为patch基本单元，对视频采用两个视频帧作为一组
4. 模型结构：RMSNorm、SwiGLU，提升视觉和语言之间的计算效率和兼容性
5. 训练：从头训练，CLIP预训练、视觉-语言对齐、端到端微调多个阶段；原生分辨率动态采样，按照图片的原始长宽比进行随机采样，从而提升模型对不同分辨率输入的泛化能力

#### 2.1.2 原生动态分辨率&帧率

#### 2.1.3 MRoPE和绝对时间对齐

### 2.2 预训练
