---
title: "Qwen-VL"
author: curya
date: 2025-10-21
categories: [Blogs,BasicKnow,LLM]
tags: [BasicKnow]
math: true
mermaid: true
---

论文标题：Qwen2.5-VL Technical Report

**核心**
- 模型能力：
    - 更强的视觉识别、精准的目标定位、稳健的文档解析、长视频理解（能够使用「边界框/点」准确定位目标，可以对发票、表单和表格等结构化数据进行稳健信息抽取，对图表、示意图、布局等进行详细分析）
    - 交互式视觉智能体：具备推理、工具使用和任务执行能力，能够胜任如操作电脑、移动设备等真实场景
- **动态分辨率处理** & **绝对时间编码**：可以处理不同分辨率的图像和时长达数小时的视频，具备秒级事件定位能力（*赋予模型原生的空间尺寸和时间动态感知能力，无需依赖传统归一化技术*）。
    - 从零训练原生动态分辨率的ViT（native dynamic-resolution Vision Transformer），并引入了窗口注意力机制，在保留原生分辨率的同时大幅降低计算开销

## 1. Introduction
主流范式：视觉编码器、跨模态投影层、LLM

面临问题：计算复杂度高、上下文理解有限、细粒度视觉感知能力弱以及在不同序列长度下性能不稳定等问题。

Qwen2.5-VL：细粒度感知能力，具体贡献如下
1. 在视觉编码器中实现了**窗口注意力机制**，以优化推理效率；
2. 引入**动态帧率采样**，将动态分辨率扩展到**时间维度**，实现对不同采样率视频的全面理解；
3. 在**时间域升级了 MRoPE**，通过对齐绝对时间，促进了更高效的时间序列学习；
4. 在**高质量数据**整理方面做出了重大努力，无论是预训练还是有监督微调，并将预训练语料库规模从 1.2T 扩展到 4.1T tokens。

Qwen2.5-VL的突出特性包括：
- 强大的文档解析能力：Qwen2.5-VL将文本识别升级为全能文档解析，在多场景、多语言及多种内嵌（如手写、表格、图表、化学公式、乐谱）文档处理方面表现卓越。
- 跨格式的精准目标定位：Qwen2.5-VL在目标检测、指认和计数方面显著提升了准确性，支持**绝对坐标**和JSON格式，实现高级空间推理。
- 超长视频理解与细粒度视频定位：模型将原生动态分辨率扩展到时间维度，能够理解时长达数小时的视频，并在秒级范围内提取事件片段。
- 增强的终端智能体能力：模型具备高级的定位、推理和决策能力，极大提升了其在手机和电脑等终端设备上的智能体功能。

## 2. 方法
视觉编码器：在结构上，融入了FFN with SwiGLU、RMSNorm、窗口注意力 --> 提升性能 & 效率
- 原生分辨率处理、动态帧率采样：不同尺寸的图像和不同帧率的视频会映射为不同长度的token序列
- MRoPE：多模态位置编码，能够在事件维度将时间ID与绝对时间对齐，使模型能够更好理解时间动态（如事件节奏、精确时刻的定位）。
<img width="2938" height="2560" alt="image" src="https://github.com/user-attachments/assets/b000a72e-ef3c-4a9a-8195-bb1dc66ba0df" />

### 2.1 模型结构
Qwen2.5-VL模型包含3个模块：
- LLM：模型初始化自Qwen2.5的预训练权重。1D RoPE --> MRoPE，实现与绝对时间对齐。
- Vision Encoder：重新设计的ViT架构。结合2D-RoPE和窗口注意力，支持原生分辨率&加速视觉编码器计算效率。训练&推理过程中，输入图像的宽高会调整为28的倍数输入，patch切分的步幅是14。
- MLP-based Vision-Language Merger：MLP映射，图像特征压缩&维度对齐。空间相邻4个patch特征分组、拼接，MLP进行映射，投影到LLM维度。

不同尺寸的Qwen2.5-VL的结构如下（视觉编码器尺寸都是一样的）：
<img width="2504" height="1816" alt="image" src="https://github.com/user-attachments/assets/04726a4c-03ac-4fa8-bdee-516d7882d8e6" />

#### 2.1.1 视觉编码器
**问题**：处理原生分辨率（有的大有的小），导致训练/推理计算负载不均衡。

**优化**：
1. 引入窗口注意力（4个层使用全局注意力，其余层使用窗口注意力，尺寸为112x112）
2. 采用2D RoPE，捕获二维空间中的位置关系
3. 视频处理：扩展到三维patch分割，14x14作为patch基本单元，对视频采用两个连续视频帧作为一组
4. 模型结构：RMSNorm、SwiGLU，提升视觉和语言之间的计算效率和兼容性 --> 和语言模型的结构设计对齐，保持一致
5. 训练：从头训练，CLIP预训练、视觉-语言对齐、端到端微调多个阶段；原生分辨率动态采样，按照图片的原始长宽比进行随机采样，从而提升模型对不同分辨率输入的泛化能力

#### 2.1.2 原生动态分辨率&帧率
空间域：Qwen2.5-VL能够动态地将不同尺寸的图像转换为对应长度的token序列。
- 使用输入图像的实际尺寸（**绝对坐标**）表示边界框、点及其他空间特征，使模型能够原生学习尺度信息，提升其处理不同分辨率图像的能力。（**但是在Qwen3-VL又换回了相对坐标**）

视频输入：Qwen2.5-VL引入了**动态帧率**（FPS）训练和**绝对时间编码**。
- 通过适应不同帧率，模型能够更好地捕捉视频内容的时间动态。
- 不同于其他方法通过增加文本时间戳或引入额外头部实现时间定位，Qwen2.5-VL提出将MRoPE的ID直接与时间戳对齐。该方案允许模型通过时间维度ID之间的间隔理解时间节奏，无需任何额外的计算开销。

#### 2.1.3 MRoPE和绝对时间对齐
MRoPE：在Qwen2-VL中提出，将位置嵌入分解为三个分量（时间、高度和宽度）
- 文本输入，三个分量使用相同的位置ID，等价于1D RoPE
- 图像输入，视觉token的时间ID保持不变，高度和宽度分量根据token在图像中的空间位置分配不同的ID
- 视频输入，视为帧序列，时间ID随每一帧递增，高度和宽度分量与静态图像保持一致

问题：Qwen2-VL中MRoPE的时间位置ID仅与输入帧数量相关，不能反映内容变化的速度或视频中事件的绝对时间。

改进：Qwen2.5-VL中，将MRoPE的时间分量对齐到绝对时间。模型通过利用时间ID之间的间隔，可以在不同FPS采样的视频之间实现一致的时间对齐学习。

### 2.2 预训练
#### 2.2.1 预训练数据
<img width="2640" height="902" alt="image" src="https://github.com/user-attachments/assets/e45d7557-71f9-42db-836f-af3b7fb8da70" />

- 数据规模：1.2T tokens (Qwen2-VL) --> **4T tokens (Qwen2.5-VL)**
- 数据类型：图像描述、图文混合数据、OCR数据、视觉知识（如名人、地标、动植物识别）、多模态学术问答、定位数据、文档解析数据、视频描述、视频定位以及基于智能体的交互数据
- 数据来源：清洗原始网页数据、合成数据等

**图文混合数据**  
图文混合数据对于多模态学习至关重要，主要带来三大益处：

（1）使模型能借助视觉与文本线索实现上下文学习；

（2）在图片缺失时，保持强大的纯文本能力；

（3）包含大量通用信息。

问题：目前可用的图文混合数据大多缺乏有意义的图文关联，且噪声较多，限制了其在复杂推理和创新生成上的作用。

解决方法：数据打分 & 清洗，确保保留高质量且相关性的图文混合数据。
- 流程一：标准数据清洗
- 流程二：内部评测模型打分，打分标准包括：（1）文本质量、（2）图文相关性、（3）图文互补性和（4）信息密度平衡。
    - 图文相关性：分数越高，图像与文本间关联越强，图像对于文本能有效补充、解释或扩展，而非仅作装饰。
    - 信息互补性：分数越高，图像和文本互为补充，两者各自提供独特细节，共同构成完整语境。
    - 信息密度平衡：分数越高，图像与文本的信息分布越均衡，避免单一维度信息过多，保证两者协同。

**绝对坐标定位数据**  
在训练时，Qwen2.5-VL基于输入图片的实际尺寸来表示边框与关键点（绝对坐标而非相对坐标），从而帮助模型更好地把握真实世界中的尺度和空间关联，提升目标检测与定位等任务的表现。

为提升模型在定位上的泛化能力，联合公开数据集和自有数据，构建了覆盖边框、关键点和指代表达的综合数据集。
- 采用多种格式（如XML、JSON和自定义）进行合成，利用复制粘贴增强及现成模型进行合成，从而实现更全面的能力评测和进步。
- 开放词汇目标检测，将训练数据类别扩展至10000+对象。此外，为提升模型在极端目标检测场景下的表现，在查询中合成了不存在的目标类别，并为每个目标类别创建含多个实例的图片数据。
- 为确保模型具备优异的点目标定位能力，构建了涵盖公开及合成数据的点定位数据集。包含PixMo公开的点标记和计数、公开的目标定位（检测和实例分割），以及通过自动化流程生成的针对图片细节精准指点数据。

**文档全能解析数据**  
- 传统文档解析：专用模型，分别处理版面分析、文本提取、图表理解、插图识别等任务。
- Qwen2.5-VL：致力于打造通用模型，实现文档结构的解析、理解和格式转化。

合成数据

在文档中融入多种元素，包括表格、图表、公式、自然或合成图片、乐谱及化学表达式，并全部统一为**HTML格式**，借助标签结构描述布局框和插图信息。
- 问题：训练的时候输入是HTML渲染出来的图像吗？HTML是用于合成数据的手段？同时也方便获取GT用于模型训练？

**OCR数据**  
合成数据、开源数据和自采数据。

- 合成数据：由视觉文本生成引擎自动生成，覆盖多样的真实场景文本图片。
- 开源数据：为支持更多语言，提升多语种能力，引入了大规模多语种OCR数据集，其中涵盖法语、德语、意大利语、西班牙语、葡萄牙语、阿拉伯语、俄语、日语、韩语和越南语等多种语言。该数据集既包含高质量合成图片，也包含真实场景自然图片，保证多文本样式和环境的丰富性，有助于模型在不同语境下展现强适应力。
- 自采数据：
    - 针对图表数据，利用matplotlib、seaborn、plotly等可视化库**合成**了100万样本，涵盖柱状图、关系图、热力图等类型。
    - 针对表格数据，基于离线端到端表格识别模型处理了600万**真实样本**，过滤出置信度低、重叠及单元格密度不足的表格。

**视频数据**  
为提升模型对不同帧率（FPS）的视频理解能力，在训练阶段动态采样FPS，实现训练样本中FPS的均匀分布。
对于超过半小时的视频，专门通过合成流程生成了多帧字幕。
针对视频锚定，采用了秒级和时分秒帧（hmsf）格式的时间戳，确保模型能准确理解和输出不同时间格式的结果。

**智能体数据**  
通过数据采集和合成增强Qwen2.5-VL的感知和决策能力，赋予智能体能力。
- 在感知部分，收集了移动端、网页、桌面端的截图，并使用合成引擎生成截图描述和界面元素定位标注。图片描述任务帮助模型理解图形界面，定位任务帮助其对齐元素外观与功能。
- 在决策部分，将手机、网页和桌面端的操作统一为具备共享动作空间的函数调用格式，并基于开源数据及智能体框架在虚拟环境下合成收集多步轨迹，统一重构为函数形式。

#### 2.2.2 预训练细节
从零训练一个ViT（使用DataComp数据集&部分自有数据集），作为视觉编码器的初始化；采用预训练好的Qwen2.5作为语言模型的初始化。预训练分为三个阶段（每个阶段采用不同数据配置和训练策略，逐步提升模型能力），如下表所示：
<img width="1720" height="600" alt="image" src="https://github.com/user-attachments/assets/85921f41-9eb3-48b4-9712-fc902af697e0" />

- **第一阶段**：仅训练ViT，以**增强其与语言模型的对齐性**（促进ViT高效提取能够与文本信息集成的有意义视觉表征），为多模态理解奠定坚实基础。此阶段主要数据来源包括图像描述、视觉知识和OCR数据。

- **第二阶段**：ViT & LLM，并使用多样化的多模态图像数据进行训练，以**提升模型处理复杂视觉信息的能力**。该阶段引入了更多复杂且具有推理要求的数据集，如图文混合数据、多任务学习数据集、视觉问答（VQA）、多模态数学、智能体相关任务、视频理解和纯文本数据集。这些数据集强化了模型在视觉与语言模态之间建立更深层次联系的能力，使其能够应对日益复杂的任务。

- **第三阶段**：ViT & LLM，为进一步提升模型在**长序列上的推理能力**，加入了视频和智能体相关数据，同时将序列长度进行扩展。这样，模型能够以更高精度处理更高级和复杂的多模态任务。序列长度的增加，使模型具备了处理更长上下文的能力，尤其有利于依赖长距离信息和复杂推理的任务。

为应对图片尺寸和文本长度变化给训练带来的计算负载不均等挑战，采用了优化训练效率的策略：基于输入到LLM中的序列长度动态进行数据样本打包，确保负载均衡。
- 在第一和第二阶段，数据统一打包为长度8,192的序列；
- 第三阶段则将序列长度扩展至32,768，以匹配模型处理长序列的能力提升。

### 2.3 后训练（SFT + DPO）
SFT：通过定向指令优化，缩小预训练表征与下游任务需求之间的差距。采用 ChatML 格式组织指令跟随数据，有意与预训练数据结构差异化。这种格式的转变带来了三项关键适配：
1) 多模态交互中的显式对话角色标记，
2) 视觉嵌入与文本指令的结构化注入，
3) 通过格式感知式打包，保持跨模态位置关系稳定。

#### 2.3.1 指令数据
数据量：包含200万条数据，纯文本数据和多模态（图文、视频-文本）数据各占50%。主要以中英文为主，辅以其它语言样本以支持多语种能力。单轮 & 多轮交互；单图像 & 多图像
- 多模态数据的引入使模型能够更好地处理复杂输入。
- 尽管纯文本和多模态数据数量相当，但由于视觉和时序信息的嵌入，多模态样本在训练中会消耗更多的tokens和算力。

#### 2.3.2 数据过滤流程 - 质量保证
两阶段的数据过滤流程：

**第一阶段**：领域分类 - 使用 Qwen2-VL-Instag（由 Qwen2-VL-72B 衍生而来的专用分类模型），对问答（QA）对进行分层归类。将 QA 对划分为8个主要领域（如编程与规划），每个主领域又进一步拆分为30个细分子类。

**第二阶段**：领域定制过滤 - 结合了基于规则和基于模型的方法，综合提升数据质量。由于文档处理、OCR、视觉锚定等领域各自特点不同，可能需采用具体的过滤策略。下述为各领域通用过滤策略简述：
- 规则过滤：采用预设启发式规则删除低质量或异常数据。特别针对文档、OCR和锚定任务，识别并去除重复模式，以防止模型学习失真。同时，去除包含不完整、截断或格式错误的响应，这类问题常见于合成及多模态数据中。为确保相关性和遵守伦理规范，任何无关或可能导致有害结果的问答也会被筛除。此结构化流程确保数据既合乎伦理也能满足任务要求。
- 模型过滤：进一步利用 Qwen2.5-VL 系列训练的奖励模型，从多个维度评估多模态 QA 对。查询部分针对复杂度和相关性，仅保留适当挑战且语境相关的示例。答案方面则要求正确、完整、清晰、紧扣问题且具备参考价值。在视觉锚定任务中，尤其关注对视觉信息的准确解读和合理利用。多维度评价保证只有高质量数据进入SFT阶段。

#### 2.3.3 拒绝采样增强推理 - 补充高质量CoT数据
为补充结构化数据过滤流程，采用**拒绝采样（Rejection Sampling）策略来优化数据集**，并增强视觉语言模型（VLM）的推理能力。

**拒绝采样**流程始于配有真实标签的数据集，这些任务需要多步推理，如数学问题求解、代码生成和领域特定的VQA。利用中间版本的Qwen2.5-VL模型，将生成的响应与真实答案进行比对。仅保留模型输出与期望答案一致的样本，确保最终数据集由高质量和准确实例组成。
- 拒绝采样利用中间Qwen2.5-VL模型生成并筛选高质量答案，保留下来的数据用于后续的SFT（监督微调）训练？

#### 2.3.4 训练
Qwen2.5-VL 的后训练流程包含两个阶段：监督微调（SFT）和直接偏好优化（DPO），这两个阶段均保持ViT参数冻结。
- 在 SFT 阶段，模型通过多样化的多模态数据进行微调，这些数据包括图文对、视频和纯文本，来源涵盖通用视觉问答（VQA）、**拒绝采样**以及文档与 OCR、锚定、视频和智能体相关任务等专项数据集。
- DPO 阶段则专注于图文和纯文本数据，利用偏好数据对模型进行优化，使其更贴近人类偏好，并且每个样本只处理一次，以确保高效优化。


Qwen3-VL改进：
- 一是采用 MRoPE-Interleave，原始MRoPE将特征维度按照时间（t）、高度（h)和宽度（w)的顺序分块划分，使得时间信息全部分布在高频维度上。在 Qwen3-VL 中采取了 t,h,w 交错分布的形式，实现对时间，高度和宽度的全频率覆盖，这样更加鲁棒的位置编码能够保证模型在图片理解能力相当的情况下，提升对长视频的理解能力；
    - [ttthhhwww] -> [thwthwthw]
- 二是引入 DeepStack 技术，**融合 ViT 多层次特征**，提升视觉细节捕捉能力和图文对齐精度；
    - 将ViT不同层的视觉特征token化作为视觉输入。这种设计能够有效保留从底层（low-level）到高层（high-level）的丰富视觉信息。（**多层ViT特征**）
        - 多层次特征提取：把ViT的第 8 / 16 / 24 层的特征提取出来
    - 将LMM单层输入视觉tokens的范式，改为在LLM的多层中进行注入。旨在实现更精细化的视觉理解。（**在LLM中多层注入**）
        - 多层特征注入：在LLM的第 0 / 1 / 2 层的 forward 结束以后，加上提取的 8 / 16 / 24 层的多层次特征
- 三是将原有的视频时序建模机制 T-RoPE 升级为 文本时间戳对齐机制。
    - 采用“时间戳-视频帧”交错的输入形式，实现帧级别的**时间信息与视觉内容的细粒度对齐**。
    - 原生支持“秒数”与“时:分:秒”（HMS）两种时间输出格式。

![Qwen3-VL架构图](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg)

