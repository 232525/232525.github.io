[ { "title": "Qwen-VL", "url": "/posts/Qwen-VL/", "categories": "Blogs, BasicKnow, LLM", "tags": "BasicKnow", "date": "2025-10-21 00:00:00 +0800", "snippet": "论文标题：Qwen2.5-VL Technical Report核心 模型能力： 更强的视觉识别、精准的目标定位、稳健的文档解析、长视频理解（能够使用「边界框/点」准确定位目标，可以对发票、表单和表格等结构化数据进行稳健信息抽取，对图表、示意图、布局等进行详细分析） 交互式视觉智能体：具备推理、工具使用和任务执行能力，能够胜任如操作电脑、移动设备等真实场景 动态分辨率处理 &amp;amp; 绝对时间编码：可以处理不同分辨率的图像和时长达数小时的视频，具备秒级事件定位能力（赋予模型原生的空间尺寸和时间动态感知能力，无需依赖传统归一化技术）。 从零训练原生动态分辨率的ViT（native dynamic-resolution Vision Transformer），并引入了窗口注意力机制，在保留原生分辨率的同时大幅降低计算开销 1. Introduction主流范式：视觉编码器、跨模态投影层、LLM面临问题：计算复杂度高、上下文理解有限、细粒度视觉感知能力弱以及在不同序列长度下性能不稳定等问题。Qwen2.5-VL：细粒度感知能力，具体贡献如下 在视觉编码器中实现了窗口注意力机制，以优化推理效率； 引入动态帧率采样，将动态分辨率扩展到时间维度，实现对不同采样率视频的全面理解； 在时间域升级了 MRoPE，通过对齐绝对时间，促进了更高效的时间序列学习； 在高质量数据整理方面做出了重大努力，无论是预训练还是有监督微调，并将预训练语料库规模从 1.2T 扩展到 4.1T tokens。Qwen2.5-VL的突出特性包括： 强大的文档解析能力：Qwen2.5-VL将文本识别升级为全能文档解析，在多场景、多语言及多种内嵌（如手写、表格、图表、化学公式、乐谱）文档处理方面表现卓越。 跨格式的精准目标定位：Qwen2.5-VL在目标检测、指认和计数方面显著提升了准确性，支持绝对坐标和JSON格式，实现高级空间推理。 超长视频理解与细粒度视频定位：模型将原生动态分辨率扩展到时间维度，能够理解时长达数小时的视频，并在秒级范围内提取事件片段。 增强的终端智能体能力：模型具备高级的定位、推理和决策能力，极大提升了其在手机和电脑等终端设备上的智能体功能。2. 方法视觉编码器：在结构上，融入了FFN with SwiGLU、RMSNorm、窗口注意力 –&amp;gt; 提升性能 &amp;amp; 效率 原生分辨率处理、动态帧率采样：不同尺寸的图像和不同帧率的视频会映射为不同长度的token序列 MRoPE：多模态位置编码，能够在事件维度将时间ID与绝对时间对齐，使模型能够更好理解时间动态（如事件节奏、精确时刻的定位）。2.1 模型结构Qwen2.5-VL模型包含3个模块： LLM：模型初始化自Qwen2.5的预训练权重。1D RoPE –&amp;gt; MRoPE，实现与绝对时间对齐。 Vision Encoder：重新设计的ViT架构。结合2D-RoPE和窗口注意力，支持原生分辨率&amp;amp;加速视觉编码器计算效率。训练&amp;amp;推理过程中，输入图像的宽高会调整为28的倍数输入，patch切分的步幅是14。 MLP-based Vision-Language Merger：MLP映射，图像特征压缩&amp;amp;维度对齐。空间相邻4个patch特征分组、拼接，MLP进行映射，投影到LLM维度。不同尺寸的Qwen2.5-VL的结构如下（视觉编码器尺寸都是一样的）：2.1.1 视觉编码器问题：处理原生分辨率（有的大有的小），导致训练/推理计算负载不均衡。优化： 引入窗口注意力（4个层使用全局注意力，其余层使用窗口注意力，尺寸为112x112） 采用2D RoPE，捕获二维空间中的位置关系 视频处理：扩展到三维patch分割，14x14作为patch基本单元，对视频采用两个连续视频帧作为一组 模型结构：RMSNorm、SwiGLU，提升视觉和语言之间的计算效率和兼容性 –&amp;gt; 和语言模型的结构设计对齐，保持一致 训练：从头训练，CLIP预训练、视觉-语言对齐、端到端微调多个阶段；原生分辨率动态采样，按照图片的原始长宽比进行随机采样，从而提升模型对不同分辨率输入的泛化能力2.1.2 原生动态分辨率&amp;amp;帧率空间域：Qwen2.5-VL能够动态地将不同尺寸的图像转换为对应长度的token序列。 使用输入图像的实际尺寸（绝对坐标）表示边界框、点及其他空间特征，使模型能够原生学习尺度信息，提升其处理不同分辨率图像的能力。（但是在Qwen3-VL又换回了相对坐标）视频输入：Qwen2.5-VL引入了动态帧率（FPS）训练和绝对时间编码。 通过适应不同帧率，模型能够更好地捕捉视频内容的时间动态。 不同于其他方法通过增加文本时间戳或引入额外头部实现时间定位，Qwen2.5-VL提出将MRoPE的ID直接与时间戳对齐。该方案允许模型通过时间维度ID之间的间隔理解时间节奏，无需任何额外的计算开销。2.1.3 MRoPE和绝对时间对齐MRoPE：在Qwen2-VL中提出，将位置嵌入分解为三个分量（时间、高度和宽度） 文本输入，三个分量使用相同的位置ID，等价于1D RoPE 图像输入，视觉token的时间ID保持不变，高度和宽度分量根据token在图像中的空间位置分配不同的ID 视频输入，视为帧序列，时间ID随每一帧递增，高度和宽度分量与静态图像保持一致问题：Qwen2-VL中MRoPE的时间位置ID仅与输入帧数量相关，不能反映内容变化的速度或视频中事件的绝对时间。改进：Qwen2.5-VL中，将MRoPE的时间分量对齐到绝对时间。模型通过利用时间ID之间的间隔，可以在不同FPS采样的视频之间实现一致的时间对齐学习。2.2 预训练2.2.1 预训练数据 数据规模：1.2T tokens (Qwen2-VL) –&amp;gt; 4T tokens (Qwen2.5-VL) 数据类型：图像描述、图文混合数据、OCR数据、视觉知识（如名人、地标、动植物识别）、多模态学术问答、定位数据、文档解析数据、视频描述、视频定位以及基于智能体的交互数据 数据来源：清洗原始网页数据、合成数据等图文混合数据图文混合数据对于多模态学习至关重要，主要带来三大益处：（1）使模型能借助视觉与文本线索实现上下文学习；（2）在图片缺失时，保持强大的纯文本能力；（3）包含大量通用信息。问题：目前可用的图文混合数据大多缺乏有意义的图文关联，且噪声较多，限制了其在复杂推理和创新生成上的作用。解决方法：数据打分 &amp;amp; 清洗，确保保留高质量且相关性的图文混合数据。 流程一：标准数据清洗 流程二：内部评测模型打分，打分标准包括：（1）文本质量、（2）图文相关性、（3）图文互补性和（4）信息密度平衡。 图文相关性：分数越高，图像与文本间关联越强，图像对于文本能有效补充、解释或扩展，而非仅作装饰。 信息互补性：分数越高，图像和文本互为补充，两者各自提供独特细节，共同构成完整语境。 信息密度平衡：分数越高，图像与文本的信息分布越均衡，避免单一维度信息过多，保证两者协同。 绝对坐标定位数据在训练时，Qwen2.5-VL基于输入图片的实际尺寸来表示边框与关键点（绝对坐标而非相对坐标），从而帮助模型更好地把握真实世界中的尺度和空间关联，提升目标检测与定位等任务的表现。为提升模型在定位上的泛化能力，联合公开数据集和自有数据，构建了覆盖边框、关键点和指代表达的综合数据集。 采用多种格式（如XML、JSON和自定义）进行合成，利用复制粘贴增强及现成模型进行合成，从而实现更全面的能力评测和进步。 开放词汇目标检测，将训练数据类别扩展至10000+对象。此外，为提升模型在极端目标检测场景下的表现，在查询中合成了不存在的目标类别，并为每个目标类别创建含多个实例的图片数据。 为确保模型具备优异的点目标定位能力，构建了涵盖公开及合成数据的点定位数据集。包含PixMo公开的点标记和计数、公开的目标定位（检测和实例分割），以及通过自动化流程生成的针对图片细节精准指点数据。文档全能解析数据 传统文档解析：专用模型，分别处理版面分析、文本提取、图表理解、插图识别等任务。 Qwen2.5-VL：致力于打造通用模型，实现文档结构的解析、理解和格式转化。合成数据在文档中融入多种元素，包括表格、图表、公式、自然或合成图片、乐谱及化学表达式，并全部统一为HTML格式，借助标签结构描述布局框和插图信息。 问题：训练的时候输入是HTML渲染出来的图像吗？HTML是用于合成数据的手段？同时也方便获取GT用于模型训练？OCR数据合成数据、开源数据和自采数据。 合成数据：由视觉文本生成引擎自动生成，覆盖多样的真实场景文本图片。 开源数据：为支持更多语言，提升多语种能力，引入了大规模多语种OCR数据集，其中涵盖法语、德语、意大利语、西班牙语、葡萄牙语、阿拉伯语、俄语、日语、韩语和越南语等多种语言。该数据集既包含高质量合成图片，也包含真实场景自然图片，保证多文本样式和环境的丰富性，有助于模型在不同语境下展现强适应力。 自采数据： 针对图表数据，利用matplotlib、seaborn、plotly等可视化库合成了100万样本，涵盖柱状图、关系图、热力图等类型。 针对表格数据，基于离线端到端表格识别模型处理了600万真实样本，过滤出置信度低、重叠及单元格密度不足的表格。 视频数据为提升模型对不同帧率（FPS）的视频理解能力，在训练阶段动态采样FPS，实现训练样本中FPS的均匀分布。对于超过半小时的视频，专门通过合成流程生成了多帧字幕。针对视频锚定，采用了秒级和时分秒帧（hmsf）格式的时间戳，确保模型能准确理解和输出不同时间格式的结果。智能体数据通过数据采集和合成增强Qwen2.5-VL的感知和决策能力，赋予智能体能力。 在感知部分，收集了移动端、网页、桌面端的截图，并使用合成引擎生成截图描述和界面元素定位标注。图片描述任务帮助模型理解图形界面，定位任务帮助其对齐元素外观与功能。 在决策部分，将手机、网页和桌面端的操作统一为具备共享动作空间的函数调用格式，并基于开源数据及智能体框架在虚拟环境下合成收集多步轨迹，统一重构为函数形式。2.2.2 预训练细节从零训练一个ViT（使用DataComp数据集&amp;amp;部分自有数据集），作为视觉编码器的初始化；采用预训练好的Qwen2.5作为语言模型的初始化。预训练分为三个阶段（每个阶段采用不同数据配置和训练策略，逐步提升模型能力），如下表所示： 第一阶段：仅训练ViT，以增强其与语言模型的对齐性（促进ViT高效提取能够与文本信息集成的有意义视觉表征），为多模态理解奠定坚实基础。此阶段主要数据来源包括图像描述、视觉知识和OCR数据。 第二阶段：ViT &amp;amp; LLM，并使用多样化的多模态图像数据进行训练，以提升模型处理复杂视觉信息的能力。该阶段引入了更多复杂且具有推理要求的数据集，如图文混合数据、多任务学习数据集、视觉问答（VQA）、多模态数学、智能体相关任务、视频理解和纯文本数据集。这些数据集强化了模型在视觉与语言模态之间建立更深层次联系的能力，使其能够应对日益复杂的任务。 第三阶段：ViT &amp;amp; LLM，为进一步提升模型在长序列上的推理能力，加入了视频和智能体相关数据，同时将序列长度进行扩展。这样，模型能够以更高精度处理更高级和复杂的多模态任务。序列长度的增加，使模型具备了处理更长上下文的能力，尤其有利于依赖长距离信息和复杂推理的任务。 为应对图片尺寸和文本长度变化给训练带来的计算负载不均等挑战，采用了优化训练效率的策略：基于输入到LLM中的序列长度动态进行数据样本打包，确保负载均衡。 在第一和第二阶段，数据统一打包为长度8,192的序列； 第三阶段则将序列长度扩展至32,768，以匹配模型处理长序列的能力提升。2.3 后训练（SFT + DPO）SFT：通过定向指令优化，缩小预训练表征与下游任务需求之间的差距。采用 ChatML 格式组织指令跟随数据，有意与预训练数据结构差异化。这种格式的转变带来了三项关键适配：1) 多模态交互中的显式对话角色标记，2) 视觉嵌入与文本指令的结构化注入，3) 通过格式感知式打包，保持跨模态位置关系稳定。2.3.1 指令数据数据量：包含200万条数据，纯文本数据和多模态（图文、视频-文本）数据各占50%。主要以中英文为主，辅以其它语言样本以支持多语种能力。单轮 &amp;amp; 多轮交互；单图像 &amp;amp; 多图像 多模态数据的引入使模型能够更好地处理复杂输入。 尽管纯文本和多模态数据数量相当，但由于视觉和时序信息的嵌入，多模态样本在训练中会消耗更多的tokens和算力。2.3.2 数据过滤流程 - 质量保证两阶段的数据过滤流程：第一阶段：领域分类 - 使用 Qwen2-VL-Instag（由 Qwen2-VL-72B 衍生而来的专用分类模型），对问答（QA）对进行分层归类。将 QA 对划分为8个主要领域（如编程与规划），每个主领域又进一步拆分为30个细分子类。第二阶段：领域定制过滤 - 结合了基于规则和基于模型的方法，综合提升数据质量。由于文档处理、OCR、视觉锚定等领域各自特点不同，可能需采用具体的过滤策略。下述为各领域通用过滤策略简述： 规则过滤：采用预设启发式规则删除低质量或异常数据。特别针对文档、OCR和锚定任务，识别并去除重复模式，以防止模型学习失真。同时，去除包含不完整、截断或格式错误的响应，这类问题常见于合成及多模态数据中。为确保相关性和遵守伦理规范，任何无关或可能导致有害结果的问答也会被筛除。此结构化流程确保数据既合乎伦理也能满足任务要求。 模型过滤：进一步利用 Qwen2.5-VL 系列训练的奖励模型，从多个维度评估多模态 QA 对。查询部分针对复杂度和相关性，仅保留适当挑战且语境相关的示例。答案方面则要求正确、完整、清晰、紧扣问题且具备参考价值。在视觉锚定任务中，尤其关注对视觉信息的准确解读和合理利用。多维度评价保证只有高质量数据进入SFT阶段。2.3.3 拒绝采样增强推理 - 补充高质量CoT数据为补充结构化数据过滤流程，采用拒绝采样（Rejection Sampling）策略来优化数据集，并增强视觉语言模型（VLM）的推理能力。拒绝采样流程始于配有真实标签的数据集，这些任务需要多步推理，如数学问题求解、代码生成和领域特定的VQA。利用中间版本的Qwen2.5-VL模型，将生成的响应与真实答案进行比对。仅保留模型输出与期望答案一致的样本，确保最终数据集由高质量和准确实例组成。 拒绝采样利用中间Qwen2.5-VL模型生成并筛选高质量答案，保留下来的数据用于后续的SFT（监督微调）训练？2.3.4 训练Qwen2.5-VL 的后训练流程包含两个阶段：监督微调（SFT）和直接偏好优化（DPO），这两个阶段均保持ViT参数冻结。 在 SFT 阶段，模型通过多样化的多模态数据进行微调，这些数据包括图文对、视频和纯文本，来源涵盖通用视觉问答（VQA）、拒绝采样以及文档与 OCR、锚定、视频和智能体相关任务等专项数据集。 DPO 阶段则专注于图文和纯文本数据，利用偏好数据对模型进行优化，使其更贴近人类偏好，并且每个样本只处理一次，以确保高效优化。Qwen3-VL改进： 一是采用 MRoPE-Interleave，原始MRoPE将特征维度按照时间（t）、高度（h)和宽度（w)的顺序分块划分，使得时间信息全部分布在高频维度上。在 Qwen3-VL 中采取了 t,h,w 交错分布的形式，实现对时间，高度和宽度的全频率覆盖，这样更加鲁棒的位置编码能够保证模型在图片理解能力相当的情况下，提升对长视频的理解能力； [ttthhhwww] -&amp;gt; [thwthwthw] 二是引入 DeepStack 技术，融合 ViT 多层次特征，提升视觉细节捕捉能力和图文对齐精度； 将ViT不同层的视觉特征token化作为视觉输入。这种设计能够有效保留从底层（low-level）到高层（high-level）的丰富视觉信息。（多层ViT特征） 多层次特征提取：把ViT的第 8 / 16 / 24 层的特征提取出来 将LMM单层输入视觉tokens的范式，改为在LLM的多层中进行注入。旨在实现更精细化的视觉理解。（在LLM中多层注入） 多层特征注入：在LLM的第 0 / 1 / 2 层的 forward 结束以后，加上提取的 8 / 16 / 24 层的多层次特征 三是将原有的视频时序建模机制 T-RoPE 升级为 文本时间戳对齐机制。 采用“时间戳-视频帧”交错的输入形式，实现帧级别的时间信息与视觉内容的细粒度对齐。 原生支持“秒数”与“时:分:秒”（HMS）两种时间输出格式。 " }, { "title": "DeepSeek-OCR论文阅读记录", "url": "/posts/DeepSeek-OCR/", "categories": "Blogs, BasicKnow, LLM", "tags": "BasicKnow", "date": "2025-10-21 00:00:00 +0800", "snippet": "论文标题：DeepSeek-OCR: Contexts Optical Compression核心： 光学2D映射（optical 2D mapping） –&amp;gt; 上下文压缩 一页包含1000个单词的图像，其视觉编码所需的视觉tokens数可以远小于编码该1000个单词所需的文本tokens数 模型构成：Encoder-Decoder结构 DeepEncoder - 在高分辨率输入下保持低激活，同时实现高压缩比，以确保最佳且可管理的视觉tokens数量。【token数量控制 - 文本token转视觉token】 DeepSeek3B-MoE-A570M 1. Introduction利用视觉模态作为文本信息有效压缩的介质 - 简单来讲，就是把文本绘制为图像，可以使用相较于原始文本对应的文本tokens更少的视觉tokens表征丰富的信息。DeepEncoder：即使使用高分辨率输入，也可以保持低激活内存和最少的视觉tokens。 通过16x卷积压缩器串行连接window attention和global attention编码器组件。 窗口注意力组件处理大量的视觉令牌；压缩器在视觉tokens进入密集全局注意力组件之前减少了token数量，实现了有效的记忆和令牌压缩。在OCR任务上进行实验验证效果。2. Related Works2.1 VLM模型中的视觉编码器类型一：双塔架构（e.g. Vary），使用并行SAM编码器增加高分辨率图像处理的视觉词汇参数。提供了可控的参数和激活内存，但需要双重图像预处理、部署复杂、训练阶段pipeline并行困难。类型二：tile-based方法（e.g. InternVL 2.0），将图像分为小的图像块进行并行计算处理图像。能够处理极高分辨率图像，但大图像的视觉token量也很大。类型三：自适应分辨率编码（e.g Qwen-VL系列），使用NaViT范式，通过基于patch的分割直接处理完整图像。可以灵活处理不同分辨率，但可能导致GPU显存溢出，序列打包也需要很长的序列长度。2.2 端到端OCR模型传统pipeline架构（单独的检测 &amp;amp; 识别模块） –&amp;gt; VLM（端到端OCR）3. 方法3.1 模型结构编码器-解码器结构的端到端VLM 编码器：提取图像特征、tokenizing、以及压缩视觉表征。SAM-Base(80M) &amp;amp; CLIP-large(300M) 解码器：基于图像tokens和提示词生成所需的结果。DeepSeek3B-MoE-A570M3.2 DeepEncoder（编码器）为了探索上下文光学压缩的可行性，视觉编码器要求如下: 能够处理高分辨率; 高分辨率下的低激活; 少的视觉tokens; 支持多分辨率输入; 中等参数计数。3.2.1 DeepEncoder结构包含两个组件： 以窗口注意力为主的视觉感知特征提取组件：SAM-Base (patch-size 16) - 低维特征提取 2-layer卷积，实现视觉token的16x下采样：每个卷积kernal为3，stride为2，padding为1，channel 256-&amp;gt;1024 具有密集全局注意力的视觉知识特征提取组件：CLIP-large - 高维特征提取 移除初始的patch embedding层，因为输入不是图像，而是前面的tokens输出 Example: 1024 x 1024的输入图像，SAM-Base输出1024/16 x 1024/16 = 4096个patch token，卷积下采样后为 4096 / 16 = 256 个tokens。3.2.2 多分辨率支持原生分辨率和动态分辨率： 原生分辨率：512×512 (64), 640×640 (100), 1024×1024 (256), 1280×1280 (400) 动态分辨率： n×640×640 tiles (local views) &amp;amp; 1024×1024 (global view) - n * 100 + 256 n×1024×1024 tiles (local views) &amp;amp; 1280×1280 (global view) - n * 256 + 400 3.3 The MoE Decoderxxx3.4 数据 OCR 1.0数据：主要包括场景图像OCR和文档OCR等传统OCR任务 30M pages PDF data：约100种语言，中文&amp;amp;英文 25M，其他语言 5M 标注： 1）Coarse annotations：直接使用fitz处理pdf，教会模型识别光学文本，特别是小语种； 2）Fine annotations：2M pages（中文&amp;amp;英文），使用layout模型和OCR模型 OCR 2.0数据：主要包括对复杂人工图像的解析任务，如常见图表、化学公式、和平面几何解析数据 通用视觉数据：主要用于在DeepSeek-OCR中注入一定的通用图像理解能力 文本数据3.5 训练流程两阶段训练：Stage 1 - 独立训练DeepEncoder；Stage 2 - 训练DeepSeek-OCR。动态分辨率Gundam-master模式的能力是用6M数据进行CPT获得的。3.5.1 训练DeepEncoder遵循Vary的方式 $^{注}$ ，利用一个紧凑的LM使用next token预测的框架训练DeepEncoder。 数据：OCR 1.0 &amp;amp; OCR 2.0，LAION采样（100M）通用数据。 训练超参：AdanW + cosine annealing scheduler，lr 5e-5，2 epochs，bs 1280，training sequence length 4096。注：Vary训练方式，在Encoder后面接了一个小的语言模型。在任务上：OCR图像则直接预测图中文字；图标数据则预测图表的python dict；自然图像则是captioning预测。3.5.2 训练DeepSeek-OCR流水线并行：划分为4部分，DeepEncoder划分2部分，Decoder（共12层）划分2部分 PP0: SAM &amp;amp; 下采样压缩器，整体视为vision tokenizer，参数冻结 PP1: CLIP，视为输入embedding层，参数未冻结 PP2: Decoder前6层，参数未冻结 PP3: Decoder后6层，参数未冻结训练细节：20 * 8 GPUs（A100 40G），DP 40，global bs 640，AdamW + step-based scheduler，初始lr 3e-5问题：怎么训练的？自回归PT？4. 评估4.1 视觉文本压缩数据集：Fox 英文文档部分，使用deepseek-ocr的tokenizer对GT文本进行分词，选择token量在600-1300的文档进行测试（total 100页） 在tiny / small模式下测试，分别只需要64 / 100个视觉tokens即可表征图像 测试Prompt：”&amp;lt;image&amp;gt;\\nFree OCR.”4.2 OCR性能DeepSeek-OCR不仅是一个实验模型，它具有很强的实践能力，可以为LLM/VLM预训练构建数据。Benchmark：OmniDocBench 结果： 仅需要100视觉tokens (640 × 640)，deepseek-ocr就超过了使用256 tokens的GOT-OCR2.0 使用400tokens (285有效tokens，1280 × 1280)，实现了SOTA标准性能 使用少于800个tokens，DeepSeek-OCR优于MinerU2.0（需要近7,000个视觉tokens） " }, { "title": "BBPE笔记记录", "url": "/posts/BBPE/", "categories": "Blogs, BasicKnow, LLM", "tags": "BasicKnow", "date": "2025-09-26 00:00:00 +0800", "snippet": "Code: https://github.com/OctopusMind/BBPE1、正则表达式分词支持多语种分词pat_str = r&quot;&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+&quot;各部分含义如下： &#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d 匹配一些英文中最常见的缩写后缀（类似于词素/词缀）： ’s（is 或 has 的缩写，如 it’s, John’s） ‘t（not 的缩写，如 can’t, won’t） ‘re（are 的缩写，如 you’re） ‘ve（have 的缩写，如 I’ve） ‘m（am 的缩写，如 I’m） ‘ll（will 的缩写，如 I’ll） ‘d（would 或 had 的缩写，如 I’d） ?[\\p{L}]+ 匹配以可选空格开始，后面连着至少一个“字母”（Unicode类别 L，涵盖所有语言的字母）。 例： hello、你好、Привет。 注意前面的空格是可选的，目的是保留词前可能的空格信息（对BPE分词很关键）。 ?[\\p{N}]+ 匹配以可选空格开始，后面连着至少一个“数字”（Unicode类别 N）。 例： 2024、３４５（全角数字也可，因范围广）。 同样空格是 token 的一部分。 ?[^\\s\\p{L}\\p{N}]+ 匹配以可选空格开始，后面是至少一个既不是空白（\\s），也不是字母（\\p{L}），也不是数字（\\p{N}）的字符。 例：标点、符号，如： !、，、@# 等。 \\s+(?!\\S) 匹配连续的空白字符（\\s+），但后面不能跟非空白字符（(?!\\S)）。即只能匹配那些在文本末尾的空格。 例：句子末尾的多个空格。 \\s+ 匹配一个或多个空白字符。用于捕获普通的空格分隔。 注意：整体顺序也很重要，整体分词时通常优先匹配前面的规则。 2、utf-8字节值到unicode可打印字符的映射用于初始化词汇表：BBPE以字节为单位，初始化词汇表大小为256（0x00 -&amp;gt; 0xFF)，将(0-&amp;gt;255)映射到unicode字符串def bytes_to_unicode(): &quot;&quot;&quot; 返回utf-8字节列表和到unicode字符串的映射。我们特别避免映射到bbpe代码所依赖的空白/控制字符。 可逆的bbpe代码在unicode字符串上工作。这意味着如果您想避免UNKs，您需要在您的词汇表中使用大量的unicode字符。 当你有一个10B的token数据集时，你最终需要大约5K才能获得良好的覆盖。这是你正常情况下的一个显著比例， 比如说，32K的词汇量。为了避免这种情况，我们希望查找表介于utf-8字节和unicode字符串之间。 &quot;&quot;&quot; # 初始化bs均为可打印字符 bs = ( # 33 - 126 list(range(ord(&quot;!&quot;), ord(&quot;~&quot;) + 1)) + \\ # 162 - 172 list(range(ord(&quot;¡&quot;), ord(&quot;¬&quot;) + 1)) + \\ # 174 - 255 list(range(ord(&quot;®&quot;), ord(&quot;ÿ&quot;) + 1)) ) cs = bs[:] n = 0 # 处理非可打印字符（映射到其他可打印的unicode字符） for b in range(2 ** 8): # 如果不在bs中，说明是非可打印字符，需要映射到其他可打印的unicode字符 (256 + n) if b not in bs: bs.append(b) cs.append(2 ** 8 + n) n += 1 # 将cs转换为unicode字符串 cs = [chr(n) for n in cs] # 构建bs到cs的映射 return dict(zip(bs, cs))3、词汇表训练从初始词表（字节的所有表示：0x00-&amp;gt;0xff）开始，统计语料中相邻词汇的共现频次，然后合并频次最高的一对作为新的词汇加入到词汇表中，迭代进行直到满足预设要求（如词汇表大小）。BPE的最小词汇是字符级别，BBPE的最小词汇是字节级别。 @staticmethod def train_tokenizer(data, vocab_size, vocab_outfile=None, merges_outfile=None): &quot;&quot;&quot; :param data: 训练文本 :param vocab_size: 保留词表的大小 :param vocab_outfile: 保存词表的文件名 :param merges_outfile: 保存合并字节的词表 &quot;&quot;&quot; if vocab_size &amp;lt; 256: raise ValueError(&quot;vocab_size must be greater than 256&quot;) # 字节到unicode字符映射 byte_encoder = bytes_to_unicode() # 正则表达式分词模式 pat_str = r&quot;&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+&quot; # 分词并转换为字节，并映射到unicode字符（通过byte_encoder） split_words = [ [byte_encoder[b] for b in token.encode(&quot;utf-8&quot;)] for token in re.findall(pat_str, data) ] # 词汇表初始化为基本的词汇，即bytes_to_unicode所包含的内容 vocab = set(byte_encoder.values()) merges = [] # 构建词汇表，直到满足所需的词汇量 while len(vocab) &amp;lt; vocab_size: print(len(vocab)) pair_freq = Counter() # 找出出现频次最多的字节对 for split_word in split_words: pair_freq.update(zip(split_word[:-1], split_word[1:])) most_common_pair = pair_freq.most_common(1)[0][0] # 更新词汇表和合并列表 new_token = most_common_pair[0] + most_common_pair[1] # 添加新词汇 vocab.add(new_token) # 记录合并操作 merges.append(most_common_pair) # 对数据执行合并 new_split_words = [] for split_word in split_words: i = 0 new_word = [] # 对于单词中的每个重字符，尝试合并 while i &amp;lt; len(split_word) - 1: # 如果(split_word[i], split_word[i + 1])与新合并的词汇一致，则添加新词汇，并跳过下一个字符 if (split_word[i], split_word[i + 1]) == most_common_pair: new_word.append(new_token) i += 2 else: new_word.append(split_word[i]) i += 1 # 边界处理，如果i等于split_word的长度-1，则添加最后一个字符 if i == len(split_word) - 1: new_word.append(split_word[i]) new_split_words.append(new_word) # 更新split_words，用于下一轮迭代 split_words = new_split_words vocab = sorted(list(vocab)) # 保存文件 if merges_outfile != None: with open(merges_outfile, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: for merge in merges: f.write(merge[0] + &quot; &quot; + merge[1] + &quot;\\n&quot;) if vocab_outfile != None: with open(vocab_outfile, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: json.dump({v: i for i, v in enumerate(vocab)}, f, ensure_ascii=False)4、字符串编解码class BBPETokenizer(nn.Module): def __init__(self, vocab_path: str, merges_path: str): super().__init__() with open(vocab_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: # 获得词表 vocab = json.load(f) with open(merges_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: # 获得合并token规则词表 merges = f.read() # 将合并存储为元组列表，删除最后一个空白行 merges = [tuple(merge_str.split()) for merge_str in merges.split(&quot;\\n&quot;)[:-1]] # token到BBPE解码索引映射 self.encoder = vocab self.decoder = {v: k for k, v in self.encoder.items()} # 字节到unicode字符映射，256个字符 self.byte_encoder = bytes_to_unicode() self.byte_decoder = {v: k for k, v in self.byte_encoder.items()} self.bbpe_ranks = dict(zip(merges, range(len(merges)))) self.cache = {} # 预标记化拆分正则表达式模式 self.pat = re.compile(r&quot;&quot;&quot; &#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d| # 常见的缩写 \\ ?\\p{L}+|\\ ?\\p{N}+| # 可选空格，后跟1+ unicode字母或数字 \\ ?[^\\s\\p{L}\\p{N}]+| # 可选空格，后面跟着1+非空白/字母/数字 \\s+(?!\\S)| # 1+空白字符，后面没有非空白字符 \\s+ # 1+空格字符 &quot;&quot;&quot;, re.X) def forward(self, text): if isinstance(text, list): # 批量编码 tokens = self.encode_batch(text) tokens = [token for row in tokens for token in row] else: # 编码字符串 tokens = self.encode(text) return torch.tensor(tokens) def bbpe(self, token): &#39;&#39;&#39; 对token应用合并规则 &#39;&#39;&#39; if token in self.cache: return self.cache[token] chars = [i for i in token] # 对于每个合并规则，尝试合并任何相邻的字符对 for pair in self.bbpe_ranks.keys(): i = 0 while i &amp;lt; len(chars) - 1: if chars[i] == pair[0] and chars[i + 1] == pair[1]: chars = chars[:i] + [&quot;&quot;.join(pair)] + chars[i + 2:] else: i += 1 self.cache[token] = chars return chars def encode(self, text: str) -&amp;gt; list[int]: &#39;&#39;&#39; 将字符串编码为BBPE token &#39;&#39;&#39; bbpe_tokens_id = [] # pattern使用要输入BBPE算法的正则表达式模式拆分文本 for token in re.findall(self.pat, text): # 将token转换为其字节表示，将字节映射到其unicode表示 token = &quot;&quot;.join(self.byte_encoder[b] for b in token.encode(&quot;utf-8&quot;)) # 对token执行bbpe合并，然后根据编码器将结果映射到它们的bbpe索引 bbpe_tokens_id.extend(self.encoder[bpe_token] for bpe_token in self.bbpe(token)) return bbpe_tokens_id def tokenize(self, text): &quot;&quot;&quot; 获得编码后的字符 :param text: 文本 :return: 返回编码后的字符 &quot;&quot;&quot; bbpe_tokens = [] # pattern使用要输入BBPE算法的正则表达式模式拆分文本 for token in re.findall(self.pat, text): # 将token转换为其字节表示，将字节映射到其unicode表示 token = &quot;&quot;.join(self.byte_encoder[b] for b in token.encode(&quot;utf-8&quot;)) # 对token执行bbpe合并，然后根据编码器获得结果 bbpe_tokens.extend(bpe_token for bpe_token in self.bbpe(token)) return bbpe_tokens def encode_batch(self, batch: list[str], num_threads=4): &#39;&#39;&#39; 将字符串列表编码为BBPE token列表 &#39;&#39;&#39; with ThreadPoolExecutor(max_workers=num_threads) as executor: result = executor.map(self.encode, batch) return list(result) def decode(self, tokens) -&amp;gt; str: if isinstance(tokens, torch.Tensor): tokens = tokens.tolist() text = &quot;&quot;.join([self.decoder[token] for token in tokens]) text = bytearray([self.byte_decoder[c] for c in text]).decode(&quot;utf-8&quot;, errors=&quot;replace&quot;) return text" }, { "title": "RoPE + YARN", "url": "/posts/RoPE-YARN/", "categories": "Blogs, BasicKnow, LLM", "tags": "BasicKnow", "date": "2025-08-20 00:00:00 +0800", "snippet": "1. RoPE（旋转位置编码）将二维向量 $(x_1, x_2)$ 绕原点旋转 $\\theta$ 角度，用矩阵乘法表示如下：\\[\\begin{pmatrix} x_1&#39; \\\\ x_2&#39; \\end{pmatrix} =\\begin{pmatrix}\\cos\\theta &amp;amp; -\\sin\\theta \\\\\\sin\\theta &amp;amp; \\cos\\theta\\end{pmatrix}\\begin{pmatrix}x_1 \\\\x_2\\end{pmatrix}\\]令 $M(\\theta)$ 表示旋转矩阵\\[M(\\theta) = \\begin{pmatrix} \\cos\\theta &amp;amp; -\\sin\\theta \\\\ \\sin\\theta &amp;amp; \\cos\\theta \\end{pmatrix}\\]对于RoPE，做的事就是在位置为 $m$ 的输入向量 $x\\in\\mathcal{R}^{D}$ 上，每两两元素（2维向量 $\\times \\frac{D}{2}$ 个），根据当前位置 $m$ 和2维向量所在位置 $i$ 实施 $m\\theta_i$ 角度的旋转。可表示如下：\\[\\]" }, { "title": "秋招记录-胡言乱语", "url": "/posts/%E7%A7%8B%E6%8B%9B%E8%AE%B0%E5%BD%95_%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/", "categories": "Daily, 日常", "tags": "Daily", "date": "2023-01-27 20:46:00 +0800", "snippet": "最后拿到的Offer： 比亚迪-规划院，深圳（3技术面+HR面） 百度-搜索策略部，北京（火鸡面帮忙内推，3技术面无HR面） 高德地图-信息业务中心，北京（3技术面+HR面） 阿里达摩院-视觉技术实验室，杭州（实习转正，转正答辩） 阿里国际-AI Business，杭州（2技术面+HR面）总结： 总的来说，前期并不顺利，而且整个人也很焦虑，焦虑来自于自己太菜的原罪，后期感觉运气好了点，靠着阿里系的撑了起来。 还有就是原本很抱期望很想拿意向的结果挂了，没什么预期的反而过了。 当时很想拿个淘天集团的意向，结果挂在了三面（三面完面试官说让HR和我聊一轮，但是很久没有上传面评更新状态，然后有一天我上官网问了下客服，紧接着下午就给我流程结束了）。 阿里国际一面的时候我一度觉得是KPI面，因为30分钟不到就结束了，没手撕没八股，最后拿到了offer，而且还只有两轮技术面。也是除了BYD外最先开奖的，一开始没打算接，后来准备拒了达摩院，就argue了一下。 达摩院当时抱了很大的期待，为此拒了百度的实习，也是基本上最后才开奖，但是开了个劝退价（虽然可能有些其他的博后补贴，金额还是挺香的，但是要联系高校导师而且也不确定自己是否hold住）。成功劝退，只能说庆幸当时没有All in。不过还是很感谢当时给了实习机会，不然秋招可能更难受。 高德地图整个面试体验都挺好的，尤其是HR面，聊了快一个小时，一般HR面可能聊个30分钟就结束了。其他有意思或者很无语的事 OPPO转岗面试面了一面，F12看状态码是过了的，但是然后就没有然后了； 快手提前批和正式批各面过一次，可以无限复活重投但由于「面试体验很差，并因此觉得它是个煞笔公司」所以就没再投过； 小米和字节都是面了一面之后，过了一个月之后流程结束； 美团更神奇，国庆节前面完HR面之后到现在，没挂也没有意向。每隔几天系统无操作就会进一次人才库，然后接着又被捞回来，最近的是上周进了人才库，今天看又捞回来了。属实没搞懂它什么意思。总感觉我背着莫名其妙的道德包袱，身边的人和我说无所谓，offer接了再说，逼签三方的时候再拒，但我可能想着考虑之后不去还是趁早拒了。" }, { "title": "四种Normalization的计算差异", "url": "/posts/%E5%9B%9B%E7%A7%8DNormalization%E7%9A%84%E8%AE%A1%E7%AE%97%E5%B7%AE%E5%BC%82/", "categories": "Blogs, DeepLearning", "tags": "Normalization, BasicKnow", "date": "2022-07-07 15:40:00 +0800", "snippet": "Refs:深度学习中的Normalization方法将输入维度记为$[N, C, H, W]$，在计算操作上，不同Normalization的主要区别在于： Batch Normalization：在Batch Size方向上，对NHW做归一化，对batch size大小比较敏感； Layer Normalization：在Channel方向上，对CHW归一化； Instance Normalization：在图像像素上，对HW做归一化，多用于风格化迁移； Group Normalization：将Channel分组–&amp;gt;[B, g, C//g, H, W]，然后再对后三个维度做归一化（和InstanceNorm和LayerNorm都相似之处）；以下针对Pytorch中不同Normalizaiotn计算示例，均忽略可学习的仿射变换参数$\\gamma$和$\\beta$。1. BatchNorm2dAPI: CLASS torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\\[y=\\frac{x-\\text{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta\\] 输入: [N, C, H, W] 输出: [N, C, H, W]在NHW上计算均值和方差，代码示例如下：# input dataN, C, H, W = 3, 5, 2, 2x = torch.rand(N, C, H, W) # [N, C, H, W]# nn.BatchNorm2d 计算bn_layer = torch.nn.BatchNorm2d(C, eps=0., affine=False, track_running_stats=False)x_out_1 = bn_layer(x) # [N, C, H, W]# 按照定义计算mean_x = x.mean((0, 2, 3)) # 在 NHW上计算均值和方差std_x = x.std((0, 2, 3), unbiased=False)x_out_2 = (x - mean_x[None, :, None, None]) / std_x[None, :, None, None]# x_out_1 应与 x_out_2 相等&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; x_out_1.view(3, 5, -1)tensor([[[ 0.5701, 1.3119, 0.6911, -1.5281], [-0.2640, 0.6958, -0.4879, 2.6233], [ 1.5883, 1.3217, -0.9401, -0.8484], [ 0.6178, 0.7098, 0.6252, -0.1542], [-1.0076, -0.6226, 0.6902, -0.9112]], [[ 1.0838, 0.4721, 1.2620, -0.9831], [-0.0582, 0.7492, -0.1682, -0.8531], [ 0.2192, -0.9547, -0.8769, -1.0408], [-1.6932, 0.2731, -1.1455, -0.9619], [-0.3389, -0.1145, -0.2434, -1.3969]], [[-1.3717, -1.0275, 0.0167, -0.4972], [-0.1614, -0.5248, 0.0912, -1.6418], [ 1.6850, -0.3543, -0.3061, 0.5070], [-0.7309, -0.5870, 1.5495, 1.4972], [-0.5570, 1.7374, 1.4123, 1.3522]]])&amp;gt;&amp;gt;&amp;gt; x_out_2.view(3, 5, -1)tensor([[[ 0.5701, 1.3119, 0.6911, -1.5281], [-0.2640, 0.6958, -0.4879, 2.6233], [ 1.5883, 1.3217, -0.9401, -0.8484], [ 0.6178, 0.7098, 0.6252, -0.1542], [-1.0076, -0.6226, 0.6902, -0.9112]], [[ 1.0838, 0.4721, 1.2620, -0.9831], [-0.0582, 0.7492, -0.1682, -0.8531], [ 0.2192, -0.9547, -0.8769, -1.0408], [-1.6932, 0.2731, -1.1455, -0.9619], [-0.3389, -0.1145, -0.2434, -1.3969]], [[-1.3717, -1.0275, 0.0167, -0.4972], [-0.1614, -0.5248, 0.0912, -1.6418], [ 1.6850, -0.3543, -0.3061, 0.5070], [-0.7309, -0.5870, 1.5495, 1.4972], [-0.5570, 1.7374, 1.4123, 1.3522]]])&quot;&quot;&quot;2. LayerNormAPI: CLASS torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, device=None, dtype=None)\\[y=\\frac{x-\\text{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]在CHW上计算均值与方差，示例代码如下： 如果输入是 [N, C, H, W]形式，即API示例中的Image ExampleN, C, H, W = 3, 5, 2, 2x = torch.rand(N, C, H, W)# nn.LayerNorm计算ln_layer = torch.nn.LayerNorm([C, H, W], eps=0., elementwise_affine=False)x_out_1 = ln_layer(x) # [N, C, H, W]# 按照定义计算x_mean = x.mean([1, 2, 3])x_std = x.std([1, 2, 3], unbiased=False)x_out_2 = (x - x_mean[:, None, None, None]) / x_std[:, None, None, None]# x_out_1 应与 x_out_2 相等&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; x_out_1.view(3, 5, -1)tensor([[[ 0.6501, 1.5364, 0.7946, -1.8568], [-0.6528, 0.1166, -0.8324, 1.6618], [ 1.2697, 0.9812, -1.4672, -1.3680], [ 0.4042, 0.4850, 0.4107, -0.2747], [-0.8916, -0.5898, 0.4390, -0.8160]], [[ 2.0325, 1.1983, 2.2755, -0.7861], [ 0.0332, 0.7719, -0.0675, -0.6942], [ 0.3477, -1.1027, -1.0066, -1.2091], [-1.2681, 0.7054, -0.7184, -0.5341], [ 0.1706, 0.3713, 0.2560, -0.7758]], [[-1.5146, -1.0963, 0.1726, -0.4520], [-0.3965, -0.6928, -0.1905, -1.6036], [ 1.5817, -0.6635, -0.6105, 0.2847], [-0.6113, -0.4826, 1.4282, 1.3815], [-0.3637, 1.4651, 1.2060, 1.1581]]])&amp;gt;&amp;gt;&amp;gt; x_out_2.view(3, 5, -1)tensor([[[ 0.6501, 1.5364, 0.7946, -1.8568], [-0.6528, 0.1166, -0.8324, 1.6618], [ 1.2697, 0.9812, -1.4672, -1.3680], [ 0.4042, 0.4850, 0.4107, -0.2747], [-0.8916, -0.5898, 0.4390, -0.8160]], [[ 2.0325, 1.1983, 2.2755, -0.7861], [ 0.0332, 0.7719, -0.0675, -0.6942], [ 0.3477, -1.1027, -1.0066, -1.2091], [-1.2681, 0.7054, -0.7184, -0.5341], [ 0.1706, 0.3713, 0.2560, -0.7758]], [[-1.5146, -1.0963, 0.1726, -0.4520], [-0.3965, -0.6928, -0.1905, -1.6036], [ 1.5817, -0.6635, -0.6105, 0.2847], [-0.6113, -0.4826, 1.4282, 1.3815], [-0.3637, 1.4651, 1.2060, 1.1581]]])&quot;&quot;&quot; 如果输入是 [N, L, C]形式，即API示例中的NLP Example（图像描述中通常是这种数据组织形式），则均值和方差均在C上进行求取，即在输入数据的最后一维上求均值和方差N, L, C = 3, 4, 5x = torch.rand(N, L, C)# nn.LayerNorm计算ln_layer = torch.nn.LayerNorm(C, eps=0., elementwise_affine=False)x_out_1 = ln_layer(x) # [N, L, C]# 按照定义计算x_mean = x.mean(-1)x_std = x.std(-1, unbiased=False)x_out_2 = (x - x_mean[:, :, None]) / x_std[:, :, None]&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; x_out_1tensor([[[-0.2380, -0.2267, 1.9469, -0.7811, -0.7011], [ 1.8029, -1.1073, -0.5569, 0.2497, -0.3884], [-1.1464, -0.3209, 1.6030, 0.6406, -0.7764], [-0.0740, -1.6507, 0.7076, 1.2997, -0.2825]], [[ 0.7822, -0.4960, 0.9142, 0.5369, -1.7373], [-1.7976, -0.0445, 0.3672, 1.2590, 0.2159], [ 0.7396, 1.1869, -1.1813, -1.2006, 0.4555], [ 1.0684, 1.0592, -1.0150, 0.1810, -1.2937]], [[-0.4093, 1.6552, 0.4399, -0.3537, -1.3320], [-0.8034, 0.9525, 1.3389, -1.2672, -0.2208], [ 0.2419, -1.4972, -0.6267, 0.4232, 1.4588], [-0.7910, -1.3169, -0.1005, 1.4134, 0.7951]]])&amp;gt;&amp;gt;&amp;gt; x_out_2tensor([[[-0.2380, -0.2267, 1.9469, -0.7811, -0.7011], [ 1.8029, -1.1073, -0.5569, 0.2497, -0.3884], [-1.1464, -0.3209, 1.6030, 0.6406, -0.7764], [-0.0740, -1.6507, 0.7076, 1.2997, -0.2825]], [[ 0.7822, -0.4960, 0.9142, 0.5369, -1.7373], [-1.7976, -0.0445, 0.3672, 1.2590, 0.2159], [ 0.7396, 1.1869, -1.1813, -1.2006, 0.4555], [ 1.0684, 1.0592, -1.0150, 0.1810, -1.2937]], [[-0.4093, 1.6552, 0.4399, -0.3537, -1.3320], [-0.8034, 0.9525, 1.3389, -1.2672, -0.2208], [ 0.2419, -1.4972, -0.6267, 0.4232, 1.4588], [-0.7910, -1.3169, -0.1005, 1.4134, 0.7951]]])&quot;&quot;&quot;3. InstanceNorm2dAPI: CLASS torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)\\[y=\\frac{x-\\text{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta\\] 输入: [N, C, H, W] 输出: [N, C, H, W] (与输入维度一致)在HW上求均值和方差，代码示例如下：N, C, H, W = 3, 5, 2, 2x = torch.rand(N, C, H, W)# nn.InstanceNorm2d计算in_layer = torch.nn.InstanceNorm2d(C, eps=0., affine=False, track_running_stats=False)x_out_1 = in_layer(x)# 根据定义计算x_mean = x.mean((-1, -2))x_std = x.std((-1, -2), unbiased=False)x_out_2 = (x - x_mean[:, :, None, None]) / x_std[:, :, None, None]&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; x_out_1.view(3, 5, -1)tensor([[[ 1.5311, 0.2455, -0.9808, -0.7958], [-0.9634, -0.1927, -0.5097, 1.6658], [ 0.6410, 1.2393, -1.3178, -0.5626], [ 1.2098, -1.1274, 0.7531, -0.8355], [-0.5653, 1.6005, 0.0223, -1.0575]], [[ 0.5257, 1.3195, -1.2969, -0.5484], [-0.7867, -1.1250, 1.3355, 0.5762], [ 1.0166, 0.2899, -1.6605, 0.3540], [ 0.3264, 1.4894, -0.7927, -1.0231], [ 1.3285, 0.4290, -0.3755, -1.3820]], [[-1.5102, -0.2778, 1.0419, 0.7461], [ 0.3678, -1.1810, -0.6277, 1.4408], [ 1.7132, -0.5482, -0.3753, -0.7897], [ 0.7189, 0.9644, -0.0879, -1.5954], [-0.2354, -0.9607, 1.6719, -0.4759]]])&amp;gt;&amp;gt;&amp;gt; x_out_2.view(3, 5, -1)tensor([[[ 1.5311, 0.2455, -0.9808, -0.7958], [-0.9634, -0.1927, -0.5097, 1.6658], [ 0.6410, 1.2393, -1.3178, -0.5626], [ 1.2098, -1.1274, 0.7531, -0.8355], [-0.5653, 1.6005, 0.0223, -1.0575]], [[ 0.5257, 1.3195, -1.2969, -0.5484], [-0.7867, -1.1250, 1.3355, 0.5762], [ 1.0166, 0.2899, -1.6605, 0.3540], [ 0.3264, 1.4894, -0.7927, -1.0231], [ 1.3285, 0.4290, -0.3755, -1.3820]], [[-1.5102, -0.2778, 1.0419, 0.7461], [ 0.3678, -1.1810, -0.6277, 1.4408], [ 1.7132, -0.5482, -0.3753, -0.7897], [ 0.7189, 0.9644, -0.0879, -1.5954], [-0.2354, -0.9607, 1.6719, -0.4759]]])&quot;&quot;&quot;4. GroupNormAPI: CLASS torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True, device=None, dtype=None)\\[y=\\frac{x-\\text{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta\\] 输入: [N, C, *] 输出: [N, C, *] (与输入维度一致)令输入维度为[N, C, H, W]，首先对C进行分组， [N, C, H, W]–&amp;gt; [N, g, C//g, H, W]，然后在C//g,H,W（即后三个维度方向）上求均值和方差，示例代码如下:N, C, H, W = 3, 6, 2, 2x = torch.rand(N, C, H, W) # [N, C, H, W]# nn.GroupNorm求解# 把 C=6，划分为 2 组gn_layer = torch.nn.GroupNorm(num_groups=2, num_channels=C, eps=0., affine=False)x_out_1 = gn_layer(x)# 按照定义求解x = x.view(N, 2, C // 2, H, W) # [N, C, H, W] --&amp;gt; [N, g, C//g, H, W]x_mean = x.mean((2, 3, 4))x_std = x.std((2, 3, 4), unbiased=False)x_out_2 = (x - x_mean[:, :, None, None, None]) / x_std[:, :, None, None, None]x_out_2 = x_out_2.view(N, C, H, W)&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; x_out_1.view(3, 6, -1)tensor([[[-0.1290, -1.5416, 1.3508, 0.0259], [ 0.8220, -1.1861, -0.8968, 0.5246], [-1.1964, -0.2531, 1.0820, 1.3977], [ 0.6300, -1.5861, -1.6701, -0.3855], [ 0.6316, 1.1035, -0.2076, 0.7945], [ 0.9343, 0.2422, -1.4284, 0.9415]], [[-1.4208, -0.4870, 0.4255, -0.7972], [ 1.8013, 0.3366, 1.8382, -0.7250], [ 0.5121, -0.9930, 0.1396, -0.6302], [ 0.2940, 0.9422, 0.2082, -0.0493], [ 1.6209, -0.2877, -1.0879, 0.6238], [-0.5238, -1.7207, 1.3058, -1.3255]], [[ 0.1376, -1.6736, 1.5494, -0.6100], [-0.3534, 0.5688, -0.2642, 0.5488], [-0.8490, 1.9884, -0.0916, -0.9512], [-0.6563, 1.4381, 1.5124, 1.1264], [-0.9688, -0.5808, 0.1888, 0.0883], [-1.2760, -0.8207, 1.0518, -1.1034]]])&amp;gt;&amp;gt;&amp;gt; x_out_2.view(3, 6, -1)tensor([[[-0.1290, -1.5416, 1.3508, 0.0259], [ 0.8220, -1.1861, -0.8968, 0.5246], [-1.1964, -0.2531, 1.0820, 1.3977], [ 0.6300, -1.5861, -1.6701, -0.3855], [ 0.6316, 1.1035, -0.2076, 0.7945], [ 0.9343, 0.2422, -1.4284, 0.9415]], [[-1.4208, -0.4870, 0.4255, -0.7972], [ 1.8013, 0.3366, 1.8382, -0.7250], [ 0.5121, -0.9930, 0.1396, -0.6302], [ 0.2940, 0.9422, 0.2082, -0.0493], [ 1.6209, -0.2877, -1.0879, 0.6238], [-0.5238, -1.7207, 1.3058, -1.3255]], [[ 0.1376, -1.6736, 1.5494, -0.6100], [-0.3534, 0.5688, -0.2642, 0.5488], [-0.8490, 1.9884, -0.0916, -0.9512], [-0.6563, 1.4381, 1.5124, 1.1264], [-0.9688, -0.5808, 0.1888, 0.0883], [-1.2760, -0.8207, 1.0518, -1.1034]]])&quot;&quot;&quot;" }, { "title": "LeetCode97-交错字符串", "url": "/posts/LeetCode97_%E4%BA%A4%E9%94%99%E5%AD%97%E7%AC%A6%E4%B8%B2/", "categories": "LeetCode", "tags": "algorithm, leetcode", "date": "2022-05-09 10:20:00 +0800", "snippet": "1. 题目描述 来源：力扣（LeetCode） 链接：https://leetcode.cn/problems/interleaving-string 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。给定三个字符串 s1、s2、s3，请你帮忙验证 s3 是否是由 s1 和 s2 交错 组成的。两个字符串 s 和 t 交错 的定义与过程如下，其中每个字符串都会被分割成若干 非空 子字符串： $s = s_1 + s_2 + … + s_n$ $t = t_1 + t_2 + … + t_m$ $ |n - m| &amp;lt;= 1 $ 交错 是 $s_1 + t_1 + s_2 + t_2 + s_3 + t_3 + …$ 或者 $t_1 + s_1 + t_2 + s_2 + t_3 + s_3 + …$注意：a + b 意味着字符串 a 和 b 连接。示例 1：输入：s1 = &quot;aabcc&quot;, s2 = &quot;dbbca&quot;, s3 = &quot;aadbbcbcac&quot;输出：true示例 2：输入：s1 = &quot;aabcc&quot;, s2 = &quot;dbbca&quot;, s3 = &quot;aadbbbaccc&quot;输出：false示例 3：输入：s1 = &quot;&quot;, s2 = &quot;&quot;, s3 = &quot;&quot;输出：true提示： 0 &amp;lt;= s1.length, s2.length &amp;lt;= 100 0 &amp;lt;= s3.length &amp;lt;= 200 s1、s2、和 s3 都由小写英文字母组成进阶：您能否仅使用 O(s2.length) 额外的内存空间来解决它?2. 题解动态规划！！！动态规划！！！（其实第一反应是用递归求解，但是超时了）如果s3 可以由 s1 和 s2 交错 组成，那么用一个二维数组DP可以比较直观的表示出交错方式。以示例 1为例，s1 = &quot;aabcc&quot;，s2 = &quot;dbbca&quot;，s3 = &quot;aadbbcbcac&quot;，交错方式可表示如下:如果路径最终能够到达二维数组右下角最后一个位置DP[-1][-1]，则说明输入有解，返回True，否则返回False。但是如何判断路径能够到达右下角位置呢？可以对数组DP中数据进行编号（初始化为-1），起始位置为0，当DP更新完成之后，判断DP[-1][-1]的值是否等于S3.length即可，如图。其中，灰色为初始化的数值，蓝色为起始位置，黄色为真正有效的交错路径，白底的则是不满足要求的可部分交错的一些位置。接下来的问题就是，如何对二维数组DP进行更新，算法流程如下： 创建二维数组DP，大小为(s2.length+1)*(s1.length+1)，所有数值初始化为-1； DP[0][0]置为0，表示起始位置； 遍历DP数组每一个元素，因为在交错路径中，只能 向右 / 向下 移动（向右表示使用S1中一个字符，向下表示使用S2中一个字符），所以对于任意位置DP[i][j]，我们只需要考虑其 左 / 上 位置的数值left / up（如果位置越界，则取其数值为-1即可）是否满足一定要求，然后对DP[i][j]进行数值更新即可： 如果数值left &amp;gt;= 0，说明当前交错路径可以到达DP[i][j-1]（即S3[left-1]可以与S1或者S2中的一个字符匹配）。这时我们需要判断路径是否可以右移到DP[i][j]，即判断S3[left]是否与S1[j-1]匹配。如果匹配，则令DP[i][j] = left + 1。 如果数值up &amp;gt;= 0，说明当前交错路径可以到达DP[i-1][j]（即S3[up-1]可以与S1或者S2中的一个字符匹配）。这时我们需要判断路径是否可以下移到DP[i][j]，即判断S3[up]是否与S2[i-1]匹配。如果匹配，则令DP[i][j] = up + 1。 整个数组DP更新完成之后，判断DP[-1][-1]是否等于S3.lenght。进阶 要求则需要创建一维数组DP，遍历时动态复用。3. 代码实现Python实现class Solution: def isInterleave(self, s1: str, s2: str, s3: str) -&amp;gt; bool: # 一些可以直接返回结果的特殊情况 if len(s1) + len(s2) != len(s3): return False if len(s1) + len(s2) == len(s3) == 0: return True # 一般情况 n1 = len(s1) n2 = len(s2) n3 = len(s3) DP = [[-1] * (n1 + 1) for _ in range(n2 + 1)] DP[0][0] = 0 for i in range(n2 + 1): for j in range(n1 + 1): if i == 0 and j == 0: continue left = DP[i][j-1] if j - 1 &amp;gt;= 0 else -1 up = DP[i-1][j] if i - 1 &amp;gt;= 0 else -1 if left &amp;gt;= 0 and s3[left] == s1[j-1]: DP[i][j] = left + 1 if up &amp;gt;= 0 and s3[up] == s2[i-1]: DP[i][j] = up + 1 return DP[-1][-1] == n3" }, { "title": "劳动节的一些照片：Nikon D750 + Micro-Nikkor 55mm F3.5", "url": "/posts/Nikon55mmF3.5%E7%85%A7%E7%89%87/", "categories": "Daily, 照片", "tags": "touchfish", "date": "2022-05-03 17:13:00 +0800", "snippet": "前段时间说好不买游戏了，然后莫名其妙又有了买镜头的欲望，于是海鲜市场入了个很早就想买的AI口的Micro-Nikkor 55mm F3.5，有一说一，真的相当满意。虽然在我的D750上对焦也是真的费眼。4月30日，在实验室改了一天的代码，貌似有了一点进展，结果当天晚上发生了件有点不爽但其实也无所谓的事情。5月1日取了镜头，然后就玩了一下午（从宿舍后面，往二食堂，然后过学园二，学园一，驿站，邮局，操场北面，绕回到宿舍，也算是春游了罢！），真好van～最近又想入Mamiya RB67，或者Bronica S2（虽然很早就想买了，但一次次的克制住了购买欲望），但真的有点贵，而且海鲜市场贩子当道，也不好判断靠不靠谱。这颗蒲公英拍了好久，一开始下楼转，看到这颗蒲公英（从公寓后面往二食堂走，那条没水的沟的北面，也就是临时浴室边上），拍了一会后发现相机快没电了（出来的时候就电不多），又回宿舍换了块电池出来，又拍了半天，但拍的确实不是特别满意，缺少一个脚架。继续走，拍了白粉色小fa；然后就转到了二食堂后面的那座桥，拍的蚂蚁，但是蚂蚁跑来跑去，极难对焦。二食堂旁边停车场，有一说一亭子上长的花确实很香，我记得是叫“紫藤”，很多蜜蜂在上面飞来飞去（当然也有可能是虫子），停车场西面的斜坡草坪上很多人，男男女女，校内踏春罢。落在地上干枯的花瓣。教一楼北面的不知道名字的黄色的花。教一楼和学园二之间那个没水的池塘边上的椅子。快递驿站南面的果树，及上面的蚂蚁，这个蚂蚁真的在各个枝条上转悠了很久，堪称“深度优先遍历”了。邮局以及门口不知道有什么用途的红色电话亭？邮局北面一棵被砍掉的很矮的树。宿舍肆意生长的一颗多肉，和没有长起来的幼崽。" }, { "title": "我的破脑子", "url": "/posts/%E6%9C%80%E8%BF%91%E7%8A%B6%E6%80%81/", "categories": "Daily, 照片", "tags": "touchfish", "date": "2022-04-28 10:02:00 +0800", "snippet": "最近觉得自己的脑子真的不太灵光了，总是记不住事情，尤其是刚发生的事情。好几次洗澡的时候头发打湿了，但是就是想不起来刚刚自己有没有洗过头还是只是把头发打湿了，有的时候摸摸脑袋能够根据头发的油腻程度判断。还有就是吃维B维C，有的时候我能确定自己刚刚没吃过，有的时候打算吃，隐约觉得自己在几分钟前或者十几分钟前是不是吃过了（但并不知道是不是真的吃过了），就会纠结还吃不吃。通常遇到这种类似的情况，纠结一会没有线索和证据实在想不起来的话，就当没有发生过。就算洗了头那就再洗一次，就算吃了，那也就再吃一次。开始怀疑自己，不会痴呆吧。麻了……昨天晚上回宿舍，下雨没伞，蹭了同级一同学的伞。谈到他们老板定的毕业要求，积分制，博士生50分毕业。CCF A刊、B刊、C刊分别是30分、15分（还是20分来着？）、8分，CCF A会、B会、C会分别是20分、8分、忘了（我就说自己这脑子是真不好）。按照他们这要求，我才28分，刚过一半，麻中麻😐。做核酸从东区回来路上拍的玻璃门，感觉还八错，自我满足。学园二四楼。又要吐槽最近这科研进展了：没啥进展，又有点焦绿🌿！最近不买游戏了，但是突然又想买镜头，尼康AIS 75-150mm F3.5（小茉莉），看了看闲鱼全是贩子；除了镜头又开始想摸120胶片相机，想看看腰平取景器到底有多惊艳，但看了看价格和闲鱼贩子我又一次短暂的克制住了。我就是个老镜头器材党垃圾佬，摸摸党（光会摸不会拍党）😂。" }, { "title": "LeetCode75-颜色分类", "url": "/posts/LeetCode75_%E9%A2%9C%E8%89%B2%E5%88%86%E7%B1%BB/", "categories": "LeetCode", "tags": "algorithm, leetcode", "date": "2022-04-20 12:22:00 +0800", "snippet": "1. 题目描述 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/sort-colors 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。给定一个包含红色、白色和蓝色、共n个元素的数组nums，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。我们使用整数0、1和2分别表示红色、白色和蓝色。必须在不使用库的sort函数的情况下解决这个问题。示例 1：输入：nums = [2,0,2,1,1,0]输出：[0,0,1,1,2,2]示例 2：输入：nums = [2,0,1]输出：[0,1,2]提示： n == nums.length 1 &amp;lt;= n &amp;lt;= 300 nums[i] 为 0、1 或 2进阶： 你可以不使用代码库中的排序函数来解决这道题吗？ 你能想出一个仅使用常数空间的一趟扫描算法吗？2. 题解最朴素的想法自然是直接排序，但是题目要求不能使用库的sort函数，而且进阶要求说明该题的时间复杂度是能够降到O(n)的！🤔所以我直接看了官方题解。（是我太菜了，知道肯定是遍历过程中进行交换，但想了一会没想出来就放弃了！）2.1 方法一：单指针 + 两次遍历nums中只有0、1、2三种数值，因此可以遍历两次： 第一次遍历把数字0交换到头部，同时使用一个 计数指针 记录被交换的0的个数（或者视为当前可交换数值位置的标记）（遍历完成之后，即表示在该指针之前的数据已经都是0了，第二次遍历时只需遍历该指针之后的数据）; 第二次遍历时，把数字1交换到头部（数字0之后，即第一次遍历的计数指针之后）。2.2 方法二：双指针 + 单次遍历方法一虽然时间复杂度也是O(n)，但是对nums遍历了两次，可以使用双指针进行数值交换使用单次遍历解决该问题。使用两个指针，ptr0用于控制数字0的交换，表示在ptr0之前的数字都是0；ptr1用于控制数字1的交换，表示在ptr1之前（ptr0及其之后）的数字都是1（遍历开始前ptr0、ptr1均初始化为0）。一般情况下示意图如下:由上图我们可以看出，ptr0所指位置有可能是已经被交换过的数值1的，因此需要处理这一特殊情况。数字1需要在所有的数字0之后，因此在进行数据交换时，需要考虑一些特殊情况。 遍历过程中，如果当前数值为1，将其交换到ptr1所在位置（值得注意的是，整个遍历过程中，都满足ptr0 &amp;lt;= ptr1）。既然ptr1的位置一定大于等于ptr0，那么在进行数字1的交换时就无需关心数值0，直接交换然后将ptr1右移一位即可，如下图所示。 但是如果当前数值为0，进行交换时，则需要考虑ptr1的位置了。存在两种情况： ptr0=ptr1，说明还没有数值1被交换，我们只需要直接交换，然后将ptr0和ptr1都右移一位即可，如图； ptr0&amp;lt;ptr1，说明在数值0后面有一部分连续的数值1。此时交换之后，原本ptr0所指的数值1被交换到了一个错误位置，需要将其再次与ptr1所指数值进行交换（将数值1再次还原到它应该在的位置），此时ptr0和ptr1都进行了一次交换，所以均需要右移一位，如图（可以概括为：数值0的一次交换导致占用了一个数值1的位置，因此需要将这个数值1再次交换到正确的位置）： 3. 代码实现3.1 方法一Python实现# 单指针，记录被移动到头部的数字下标，遍历两次# 第一次将 0 移动到头部，第二次将 1 移动到头部class Solution: def sortColors(self, nums: List[int]) -&amp;gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; ptr = 0 # 计数指针，也表征当前可交换位置 n = len(nums) # 把所有的 0 移动到头部，记录头部位置 for i, num in enumerate(nums): if num == 0: nums[i], nums[ptr] = nums[ptr], nums[i] ptr += 1 # 把剩下的（计数指针之后）数据中所有的 1 移动到头部 for i in range(ptr, n): if nums[i] == 1: nums[i], nums[ptr] = nums[ptr], nums[i] ptr += 13.2 方法二Python实现# 双指针，分别用于交换 0 和 1class Solution: def sortColors(self, nums: List[int]) -&amp;gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; ptr0 = ptr1 = 0 for i, num in enumerate(nums): if num == 1: # 把数字 1 移动到头部 nums[i], nums[ptr1] = nums[ptr1], nums[i] ptr1 += 1 elif num == 0: # 把数字 0 移动到头部，此处存在两种情况： # 1、数字 0 后面已经有了一部分已经被移动到头部的连续的 1 # 2、数字 0 后面没有被移动的 1 nums[i], nums[ptr0] = nums[ptr0], nums[i] if ptr0 &amp;lt; ptr1: # 对应情况1，在进行第一次交换后，数字0后的一个1被交换到了位置i，因此需要再次交换，将其放置于连续的1后面 nums[i], nums[ptr1] = nums[ptr1], nums[i] ptr0 += 1 # 此处 ptr1 也需要 +1 的原因是： # 情况1下，第二次交换理应使ptr1 + 1； # 情况2下，数字0在首位，所以ptr1 必须得大于等于ptr0，因此同步+1 ptr1 += 1" }, { "title": "LeetCode61-旋转链表", "url": "/posts/LeetCode61_%E6%97%8B%E8%BD%AC%E9%93%BE%E8%A1%A8/", "categories": "LeetCode", "tags": "algorithm, leetcode", "date": "2022-04-18 20:00:00 +0800", "snippet": "1. 题目描述 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/rotate-list 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。给你一个链表的头节点head，旋转链表，将链表每个节点向右移动k个位置。示例 1：输入：head = [1,2,3,4,5], k = 2输出：[4,5,1,2,3]示例 2：输入：head = [0,1,2], k = 4输出：[2,0,1]提示： 链表中节点的数目在范围[0, 500]内 -100 &amp;lt;= Node.val &amp;lt;= 100 0 &amp;lt;= k &amp;lt;= 2 * 10^92. 题解科研没有进展，也就只好刷题了！题目 将链表每个节点向右移动k个位置，其实可以等价为 把链表末尾的k个节点接到原链表的头节点前。但也存在一个问题，k的大小是可能大于链表节点个数的，当大于链表节点个数时，需要进行取余计算，如下图所示（橙色覆盖的节点表示被移动到头节点的末尾节点）：因此需要知道如下信息： 链表中节点的个数m 变换后的k值，即_k = k % m 链表末尾的_k个节点的位置因此，我的第一反应就是用 栈 进行实现。遍历链表，节点入栈同时进行节点计数得到m；到达链尾，计算_k = k % m；然后出栈_k个节点，并进行指针重设。算法流程： 在原本的链表头部，新增一个头节点，方便后续操作，并创建两个指针（头指针head_ptr，尾指针tail_ptr）均指向这个新的头节点； 创建一个栈用于存放链表节点，通过尾指针遍历链表并把遍历到的节点入栈，同时进行节点计数； 链表遍历完成之后得到链表长度，计算真正需要移动到头部的节点个数_k； 出栈_k个节点，此时栈中最后一个节点（令其为stack[-1]）应设置为尾节点，而其原本的下一个节点应被设置为头节点，并将遍历得到的尾指针tail_ptr所指节点链接到head_ptr指向的下一个节点。操作如下：# 在进行指针操作之前，head_ptr指向新增的头节点，tail_ptr指向原链表的尾节点，# stack[-1]为应移动到头部的_k个节点的前一个节点# stack[-1]的下一个节点应被设置为新的头节点new_head，并作为函数返回值new_head = stack[-1].next# stack[-1]本身应被设置为新的尾节点，因此其next应被重置为nullstack[-1].next = tail_ptr.next# 原链表的尾节点应链接到原链表的头节点之前tail_ptr.next = unk_head.next3. 代码实现Python实现# Definition for singly-linked list.class ListNode: def __init__(self, val=0, next=None): self.val = val self.next = next class Solution: def rotateRight(self, head: Optional[ListNode], k: int) -&amp;gt; Optional[ListNode]: # 特殊情况，无节点，直接返回 if not head: return head unk_head = ListNode(0, head) head_ptr = unk_head tail_ptr = unk_head cnt = 0 stack_list = [] # 入栈 while tail_ptr.next != None: tail_ptr = tail_ptr.next stack_list.append(tail_ptr) cnt += 1 _k = k % cnt # 特殊情况，_k=0，无需操作，直接返回 if _k == 0: return head # 出栈 for i in range(_k): stack_list.pop() # 指针重设 new_head = stack_list[-1].next stack_list[-1].next = tail_ptr.next tail_ptr.next = unk_head.next return new_head" }, { "title": "摸鱼拍照的日子", "url": "/posts/%E5%9B%BD%E7%A7%91%E5%A4%A7%E5%B0%81%E6%A0%A1%E6%91%B8%E9%B1%BC%E6%97%A5%E5%B8%B8/", "categories": "Daily, 照片", "tags": "touchfish", "date": "2022-04-18 15:22:00 +0800", "snippet": "最近科研十分不顺利，几乎没有进展，整的我现在也不晓得咋办了（没进展的时候就只好去刷LeetCode）！我就是个FIVE！周六（2022年4月16日，农历壬寅年三月十六）晚上看到操场上好多人，闲得无聊下去凑了凑热闹，才知道是我们学院的什么低配版音乐节。看了一会准备回宿舍，反身发现当天月亮好大好圆挂在宿舍楼上，果然十五的月亮十六圆，然后回了宿舍拿了相机下楼到处瞎拍了一会。周天请了假出校理了个发，原本潦草的生活并没有因为理发而变得不潦草，但希望可以转运，科研转运！在校门口拍了几张照片，哈罗单车灵异撞树事件（不知道是哪些没素质的小兄dei整的），回了宿舍之后从3点左右睡到5点左右？然后看到操场有人放风筝，有几个放的还挺高的，我以为今天有风也带着风筝下去了，结果下去没一会啥风都没了，又收起了风筝回了宿舍。最近想吃巧克力，看了看瑞士莲觉得我不配（有点贵）放弃了，只想吃点黑巧克力，多带点苦味的内种。因为周天下午睡了，导致昨天晚上失眠了，也可能是玩手机玩的。周末看了《异物志》，觉得挺好的。前段时间看了《追爱家族》，剧情烂的莫名其妙，但看都看了所以还是看完了。还因为看到了《金宵大厦》第二季开播，去看了《金宵大厦》第一季，很久之前看了一两集，因为是饭点看的，觉得饭点看这种带点惊悚的不合适下饭就没看下去，这回看完了觉得也挺好的，剧情挺有意思的。校门口不知道叫啥名字的花: $\\downarrow\\downarrow\\downarrow$" }, { "title": "清明节前后", "url": "/posts/%E6%B8%85%E6%98%8E%E8%8A%82%E5%89%8D%E5%90%8E%E6%97%A5%E5%B8%B8/", "categories": "Daily, 照片", "tags": "touchfish", "date": "2022-04-07 18:54:00 +0800", "snippet": "清明节假期，玩了游戏，放了风筝（虽然没放起来）。生活仍然平平无奇自己害挺满意的一张照片，去实验室路上，看到树的影子投到墙上还挺有感觉的，随手拍了一张，裁了裁，调了色（瞎调，但是调完觉得挺有胶片内味，所以就挺满意）。自娱自乐第一名！" }, { "title": "LeetCode32-最长有效括号", "url": "/posts/LeetCode32_%E6%9C%80%E9%95%BF%E6%9C%89%E6%95%88%E6%8B%AC%E5%8F%B7/", "categories": "LeetCode", "tags": "algorithm, leetcode", "date": "2022-04-07 13:46:00 +0800", "snippet": "1. 题目描述 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/longest-valid-parentheses 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。给你一个只包含&#39;(&#39;和&#39;)&#39;的字符串，找出最长有效（格式正确且连续）括号子串的长度。示例 1：输入：s = &quot;(()&quot;输出：2解释：最长有效括号子串是 &quot;()&quot;示例 2：输入：s = &quot;)()())&quot;输出：4解释：最长有效括号子串是 &quot;()()&quot;示例 3：输入：s = &quot;&quot;输出：0提示：0 &amp;lt;= s.length &amp;lt;= 3 * 104s[i] 为 &#39;(&#39; 或 &#39;)&#39;2. 题解2.1 个人思路（比较菜，非官方题解，官方题解更🐂🍺，复杂度更低）思路: 涉及到括号匹配，第一反应就是使用 栈 ，遍历字符串，左括号(则入栈，右括号)则判断是否能匹配并出栈一个左括号(，这样可以很简单的找出来输入字符串中有多少对能够匹配上的括号。但是，题目要求的是找出 最长有效（格式正确且连续）括号子串的长度，只找出能够匹配的括号对数是没办法保证它们之间是连续的。 例如：对于字符串&quot;(()(()&quot;，通过入栈出栈操作，我们可以得到该字符串中有2对能够匹配的括号，但是2对括号之间并不连续，因此实际的最长有效括号子串的长度为2而不是4。那么如何在找到的匹配括号对的基础上，去计算真正的最长有效括号子串长度呢？通过判断它们是否连续即可！ 继续以字符串&quot;(()(()&quot;为例，找到的两对括号位置为[[1, 2], [4, 5]]，它们之间少了个3，因此并不连续（被分成了2个块），所以不能计算整体的长度，而应该选择2个块中长度较长者作为输出。2.2 算法流程 创建两个栈，命名为stack_left和stack_right，分别用来存储左括号信息和匹配到的完整括号信息； 遍历输入字符串，如果当前字符为左括号(，则将其下标推入stack_left；如果当前字符为右括号)，则判断stack_left中是否为空，如不为空，则出栈一个左括号(的下标，并将匹配到的左右括号的下标都推入stack_right；如果stack_left为空，则丢弃改字符，继续循环； 遍历完成之后，我们得到了包含所有匹配完整的括号的下标信息stack_right，对其中的下标按照升序进行排序（时间复杂度不够低的原因）；然后遍历排序后的stack_right，查找断点（即计算每一个连续块的子串长度），获取最长有效括号子串长度（从每个连续块子串长度中选择最长的作为输出）。3. 代码实现Pythonclass Solution: def longestValidParentheses(self, s: str) -&amp;gt; int: # 使用 栈 进行判断？ stack_left = [] stack_right = [] for idx, c in enumerate(s): if c == &#39;(&#39;: # 左括号: 入栈 stack_left.append(idx) continue else: # 右括号: 判断栈中是否存在与之匹配的左括号，存在则 tmp_len + 2，并出栈一个左括号 # 否则判断 max_len 与 tmp_len 的大小，并重置 tmp_len = 0 if len(stack_left) != 0: # stack_right.append([stack_left[-1], idx]) stack_right.append(stack_left[-1]) stack_right.append(idx) stack_left.pop() # 排序 stack_right.sort() if len(stack_right) == 0: return 0 # 断点 max_len = 0 tmp_len = 1 for i in range(1, len(stack_right)): if stack_right[i] - stack_right[i-1] == 1: tmp_len += 1 else: max_len = max(max_len, tmp_len) tmp_len = 1 max_len = max(max_len, tmp_len) return max_lenC++我又没写！" }, { "title": "如何摸鱼", "url": "/posts/%E6%91%B8%E9%B1%BC/", "categories": "Daily", "tags": "touchfish", "date": "2022-04-06 18:48:00 +0800", "snippet": "我爱摸鱼" }, { "title": "LeetCode31-下一个排列", "url": "/posts/LeetCode31_%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%8E%92%E5%88%97/", "categories": "LeetCode", "tags": "algorithm, leetcode", "date": "2022-04-05 00:00:00 +0800", "snippet": "1. 题目描述 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/next-permutation 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。整数数组的一个 排列 就是将其所有成员以序列或线性顺序排列。 例如，arr = [1,2,3] ，以下这些都可以视作 arr 的排列：[1,2,3]、[1,3,2]、[3,1,2]、[2,3,1] 。整数数组的 下一个排列 是指其整数的下一个字典序更大的排列。更正式地，如果数组的所有排列根据其字典顺序从小到大排列在一个容器中，那么数组的 下一个排列 就是在这个有序容器中排在它后面的那个排列。如果不存在下一个更大的排列，那么这个数组必须重排为字典序最小的排列（即，其元素按升序排列）。 例如，arr = [1,2,3] 的下一个排列是 [1,3,2] 。 类似地，arr = [2,3,1] 的下一个排列是 [3,1,2] 。 而 arr = [3,2,1] 的下一个排列是 [1,2,3] ，因为 [3,2,1] 不存在一个字典序更大的排列。给你一个整数数组 nums ，找出 nums 的下一个排列。必须 原地 修改，只允许使用额外常数空间。示例 1：输入：nums = [1,2,3]输出：[1,3,2]示例 2：输入：nums = [3,2,1]输出：[1,2,3]示例 3：输入：nums = [1,1,5]输出：[1,5,1]提示： 1 &amp;lt;= nums.length &amp;lt;= 100 0 &amp;lt;= nums[i] &amp;lt;= 1002. 题解以arr=[1,2,3]为例，其字典序排列如下：[1,2,3], [1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]可以发现，一般情况下，下一个排列所组成的数值都要比当前排列要大（如[1,3,2] &amp;gt; [1,2,3]）；如果当前排列是最后一个排列（[3,2,1]），则下一个排列（即第一个排列，[1,2,3]）可视为当前排列的反序输出。因此，此题的目的可以描述为：针对一个当前序列arr=[x0,x1,x2,x3,...,xn]，一般情况下，需要找到一个新的序列arr&#39;=[y0,y1,y2,y3,...,yn]，同时需要满足arr&#39;&amp;gt;arr，且arr&#39;需要尽可能的小。算法流程如下（废话连篇版）：给定当前序列arr=[x0,x1,x2,x3,...,xn]，以[1,3,5,4,2]为例 从右到左遍历序列arr，找到第一个满足x(i) &amp;lt; x(i+1)的位置i。针对示例，3&amp;lt;5满足要求，因此我们找到了位置i=1，即x1=3。这样做的目的在于：对于xi右边的子序列[x(i+1),...,xn]，左数均比右数大，因此这个子序列是没有变大的空间的，它的下一个排列只能是[xn,...,x(i+1)]。但是找到的数字x(i)，则可以用其右边的某一个比它大的数与之交换，整个序列就变大了。 第1步中，我们找到了一个较小的数x(i)，现在则需要从其右边的子序列[x(i+1),...,xn]中找到最接近x(i)且大于x(i)的数x(j)，同样通过从右到左遍历序列arr[i:]=[x(i+1),...,xn]，找到第一个满足x(j) &amp;gt; x(i)的位置j，并进行数据x(i)和x(j)的交换。针对示例，4&amp;gt;3满足要求，因此找到了位置j=3，即x3=4。这样做的目的在于：要获取下一个序列，因此需要大于当前序列，但又不能太大，从第1步可知，arr[i:]=[x(i+1),...,xn]序列是降序排列的，因此从右往左遍历找到的第一个大于x(i)的值即满足要求。 现在得到了新的序列[x0,x1,...,x(j),x(i+1)..,x(j-1),x(i),x(j+1),...,xn]，我们可以确定这个序列肯定比当前序列arr=[x0,x1,...,x(i),x(i+1)..,x(j-1),x(j),x(j+1),...,xn]要大（因为x(j)&amp;gt;x(i)），但会不会大过头了？因此我们还需要判断子序列[x(i+1)..,x(j-1),x(i),x(j+1),...,xn]是不是足够小（把这个子序列变为升序排列就足够小了！）。从第1第2步我们可以知道，初始序列的子序列[x(i+1)..,x(j-1),x(i),x(j+1),...,xn]是降序排列的，显然不够小，那交换之后的子序列x(i+1)..,x(j-1),x(i),x(j+1),...,xn]呢？也是降序排列的，所以我们只需要把这部分子序列反转就可以得到最终的结果了。算法流程如下（图示版）：以[1,3,5,4,2]为例 步骤1:从右到左遍历，找到第一个找到第一个满足nums[i] &amp;lt; nums[i+1]的位置，即nums[1]=3; 步骤2:从子序列中，从右到左遍历，找到第一个满足nums[j]&amp;gt;nums[1]=3的位置，即nums[3]=4 步骤3: 将后面的子序列反转，变为升序排列，得到最终结果[1,4,2,3,5]3. 代码实现Python实现class Solution: def nextPermutation(self, nums: List[int]) -&amp;gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; left = len(nums) - 1 - 1 while left &amp;gt;= 0 and nums[left] &amp;gt;= nums[left+1]: left -= 1 if left &amp;gt;= 0: right = len(nums) - 1 # 找到从右往左第一个比left值大的数 while right &amp;gt;= left and nums[right] &amp;lt;= nums[left]: right -= 1 # 交换 nums[left], nums[right] = nums[right], nums[left] # 反转left后的数值 left, right = left + 1, len(nums) - 1 while left &amp;lt; right: nums[left], nums[right] = nums[right], nums[left] left += 1 right -= 1C++实现我还没写" }, { "title": "Jupyter Lab配置远程访问及虚拟环境", "url": "/posts/JupyterLab%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E4%B8%8E%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/", "categories": "Blogs, 环境配置", "tags": "环境配置", "date": "2022-03-25 00:00:00 +0800", "snippet": "1. 安装jupyterconda install jupyter jupyterlab2. 生成配置文件并进行修改jupyter lab --generate-config会在用户目录~/.jupyter/下生成文件jupyter_lab_config.py，对其进行修改，涉及以下部分：c.ServerApp.ip = &#39;*&#39; # 监听所有IP# 将以下取消注释c.ExtensionApp.open_browser = Falsec.LabServerApp.open_browser = Falsec.LabApp.open_browser = Falsec.ServerApp.open_browser = False# 设置监听端口c.ServerApp.port = xxxx3. 设置密码jupyter-lab password输入密码后，会在用户目录~/.jupyter/下生成文件jupyter_server_config.json，其中存储了设置密码所生成的hash字符串。4. 将虚拟环境导入到jupyter lab中# 激活虚拟环境，xxx表示虚拟环境的名称conda activate xxx# 安装ipykernelconda install ipykernel# 加入虚拟环境python -m ipykernel install --user --name=xxx其他方法NOTE: 对于版本低于3.0.0的jupyter lab，按照如下设置应该也可以，但是高于3.0.0的版本（这里我装上的是3.2.8）照如下设置，启动jupyter lab后仍然会打开浏览器4.0 设置密码# 进入pythonpython&amp;gt;&amp;gt;&amp;gt; from notebook.auth import passwd&amp;gt;&amp;gt;&amp;gt; passwd()复制生成的字符串序列4.1 生成配置文件并修改 jupyter notebook --generate-config会在用户目录~/.jupyter/下生成文件jupyter_notebook_config.py，对其进行修改，涉及以下部分： c.NotebookApp.ip = &#39;*&#39; c.NotebookApp.open_browser = False c.NotebookApp.port = xxxx c.NotebookApp.password = &#39;设置密码生成的字符串&#39;" }, { "title": "树莓派4B（8G内存版本）安装配置Ubuntu Mate 20.04（arm64）记录", "url": "/posts/%E6%A0%91%E8%8E%93%E6%B4%BE4B_8G%E5%86%85%E5%AD%98%E7%89%88%E6%9C%AC_%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEUbuntu_Mate_20.04_arm64_%E8%AE%B0%E5%BD%95/", "categories": "Blogs, 环境配置", "tags": "环境配置", "date": "2022-01-18 00:00:00 +0800", "snippet": "1. 镜像烧录1.1 相关资源下载从树莓派官网中，下载树莓派镜像烧录（Raspberry Pi Imager）软件；前往Ubuntu Mate官网，下载系统镜像。（烧录Raspberry Pi OS aarch64系统同样可以，环境配置部分的操作都可以通用，验证过了）1.2 镜像烧录打开树莓派镜像烧录软件，将插入了SD卡的读卡器插入电脑。选择下载的系统镜像，及插入的SD卡，点击烧录，等待烧录完成。2. 系统安装与基本配置2.1 系统安装将烧录好的SD卡插入到树莓派中，并插入鼠标键盘方便后续操作，对树莓派上电，等待自启动，并进行一些简单设置（语言、位置、网络、用户名及密码等），等待系统安装完成。2.2 系统基本配置2.2.1 更新需保证网络正常，然后进行更新sudo apt updatesudo apt upgrade如果出现报错，先将树莓派重启sudo reboot，再次更新即可。2.2.2 安装开启ssh服务并设置开机自启sudo apt install openssh-serversudo service ssh startsudo systemctl enable ssh查看本机ip地址sudo apt install net-toolsifconfigssh连接树莓派，其中user_name为系统用户名，xxx.xxx.xxx.xxx为IP地址ssh user_name@xxx.xxx.xxx.xxx2.2.3 安装开启VNC服务并设置开机自启未完…2.2.4 安装树莓派官方系统中的raspi-config树莓派官方的Raspberry Pi OS中预安装了raspi-config，某些设置相对比较方便，在其他第三方系统中则需要自行安装，首先从其官网下载安装文件，下载地址：archive.raspberrypi.org/debian/pool/main/r/raspi-config此处，我下载的为raspi-config_20220112_all.deb 安装相关依赖 sudo apt install whiptail parted lua5.1 alsa-utils psmisc 安装raspi-config sudo dpkg -i raspi-config_20220112_all.deb 运行raspi-config sudo raspi-config 2.2.5 安装中文输入法 sudo apt-get install fcitx fcitx-googlepinyin fcitx-module-cloudpinyin fcitx-sunpinyin然后进入系统设置，首先进入&quot;System Settings-&amp;gt;Personal-&amp;gt;Language Support&quot;，将&quot;Keyboard input method system&quot;修改为&quot;fcitx&quot;；然后进入&quot;System Settings-&amp;gt;Other-&amp;gt;Fcitx Configuration&quot;，点击+号，取消勾选Only Show Current Language，然后搜索找出Google Pinyin，点击OK添加即可。ctrl+space进行输入法的切换。2.2.6 验证USB摄像头是否可用树莓派有官方的CSI接口摄像头（但我没有），具体怎么使用还不知道（没看过资料，只大概知道需要先在raspi-config中开启摄像头，使用raspistill命令可以捕获图像）。但我手上只有一条USB接口的摄像头，需要验证该摄像头是否可以在树莓派上正常使用# 插入USB摄像头，查看其设别号（插入和拔下分别查看设备号，消失的就是摄像头的了）ls /dev/video*# 安装fswebcamsudo apt install fswebcam# 捕获摄像头图片，存储到/PATH/OF/img.jpg，# 此处我的摄像头设备号为/dev/video0fswebcam /dev/video0 /PATH/OF/img.jpg2.2.7 系统镜像备份将SD卡从树莓派上取下，插入读卡器，并插入到另一台Ubuntu操作系统的计算机上。参考了用树莓派4b构建深度学习应用（二）软件篇# 1 查看U盘的盘符，在此处我的U盘为/dev/sdglsblk# 2 利用dd命令将SD卡中的数据保存为.img镜像文件sudo dd if=/dev/sdg of=/PATH/OF/rpi_arm64_backup.img bs=8M# 3 缩小.img镜像文件尺寸git clone https://github.com/Drewsif/PiShrinkcd PiShrink/sudo ./pishrink.sh -s /PATH/OF/rpi_arm64_backup.img /PATH/OF/rpi_arm64_backup_small.img未完3. 其他开发环境配置3.0 Python虚拟环境管理（Miniforge安装）&amp;gt;1. Miniforge的安装一直使用Anaconda作为python虚拟环境管理工具，但是Anaconda和Miniconda对树莓派的支持并不好，两者的官网上倒是都有支持aarch64的包可以下载，但是我都试了一些有问题。所以决定使用Miniforge进行替代，暂时感觉使用正常。前往清华镜像站下载Miniforge，此处我下载的是Miniforge3-4.11.0-0-Linux-aarch64.sh；进行安装：cd Downloads# 安装过程最后，Do you wish the installer to initialize Miniforge3 by running conda init? [yes|no]# 我选择的是yes，选择no的话可能需要自己将环境变量添加到.bashrc中bash Miniforge3-4.11.0-0-Linux-aarch64.shsource ~/.bashrc&amp;gt;2. 安装numpyconda install numpy&amp;gt;3. 创建虚拟环境# 此处安装的Miniforge的base环境下的python为3.9.7# 新建一个python3.7的虚拟环境raspi，用于后续的环境配置conda create -n raspi python=3.7# 激活虚拟环境conda activate raspi# 安装numpy、pyyamlconda install numpy pyyaml3.1 OpenCV 4.5.5的编译与安装3.1.1 使用pip安装release版本由于树莓派为arm架构，本来打算手动编译安装OpenCV，因为conda安装找不到相关的包。但是后来尝试了pip安装，结果发现官方有编译好的包可以直接安装，见pypi.org/project/opencv-contrib-python。（所以，手动编译安装OpenCV就没太大必要了）# 进入raspi虚拟环境conda activate raspi# 以下四种应该都可以，此处我安装的是opencv-contrib-pythonpip install opencv-contrib-pythonpip install opencv-pythonpip install opencv-python-headlesspip install opencv-contrib-python-headless如下图，网站上显示，支持aarch64架构的opencv对应的Python Version为3.6，但我虚拟环境是python3.7，测试了一下读取图片是没有问题的。3.1.2 从源码进行编译安装$\\color{red}{NOTE: 以下部分为非必要操作}$NOTE：还未在Ubuntu Mate上测试（待测试），在Raspberry Pi OS 32位上测试过能编译成功。参考了用树莓派4b构建深度学习应用（三）OpenCV篇和Ubuntu 16.04编译配置opencv 4.1.1 + opencv_contrib 4.1.1（C++ &amp;amp; Python）&amp;gt;1. 依赖库的安装sudo apt -y install build-essential cmake unzip pkg-configsudo apt -y install libjpeg-dev libpng-dev libtiff-devsudo apt -y install libavcodec-dev libavformat-dev libswscale-dev libv4l-devsudo apt -y install libxvidcore-dev libx264-devsudo apt -y install libgtk-3-devsudo apt -y install libcanberra-gtk*sudo apt -y install libatlas-base-dev gfortran&amp;gt;2. 构建、编译OpenCV（待测试）首先从OpenCV的官方github库中下载opencv-4.5.5和opencv_contrib-4.5.5的源码。构建cd opencv-4.5.5mkdir build cd buildcmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local/opencv-4.5.5 \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D WITH_TBB=ON \\ -D WITH_V4L=ON \\ -D WITH_QT=OFF \\ -D WITH_OPENGL=OFF \\ -D WITH_FFMPEG=ON \\ -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-4.5.5/modules \\ -D BUILD_EXAMPLES=ON \\ -D ENABLE_NEON=ON \\ -D ENABLE_VFPV3=ON \\ -D OPENCV_ENABLE_NONFREE=ON \\ -D CMAKE_SHARED_LINKER_FLAGS=&#39;-latomic&#39; \\ -D OPENCV_GENERATE_PKGCONFIG=YES ..编译make -j4安装sudo make install未完3.2 PyTorch及TorchVision的编译与安装3.2.1 安装官方或第三方编译release的whl文件有大佬提供了aarch64的release版本，下载地址https://github.com/KumaTea/pytorch-aarch64/releases，此处我下载的分别是： torch-1.7.1-cp37-cp37m-linux_aarch64.whl torchaudio-0.7.2-cp37-cp37m-linux_aarch64.whl torchvision-0.8.2-cp37-cp37m-linux_aarch64.whl(1.7.1版本安装后报错，找不到libpython3.7m.co.1.0；改为1.8.1版本后正常) torch-1.8.1+ffmpeg-cp37-cp37m-linux_aarch64.whl torchvision-0.9.1+ffmpeg-cp37-cp37m-linux_aarch64.whl torchaudio-0.8.1-cp37-cp37m-linux_aarch64.whl安装：# 进入raspi虚拟环境conda activate raspi# 安装pip install torch-1.8.1+ffmpeg-cp37-cp37m-linux_aarch64.whl pip install torchvision-0.9.1+ffmpeg-cp37-cp37m-linux_aarch64.whl pip install torchaudio-0.8.1-cp37-cp37m-linux_aarch64.whlNOTE: 测试了一下vgg16前向计算，感觉速度过于慢了，感觉不太正常！从源码编译安装试试（待完成）***仔细找了一下，发现pytorch官方其实发布了各个版本编译的whl文件，其中就包括了aarch64版本的，下载地址:https://download.pytorch.org/whl/cpu/torch_stable.html，我从中下载安装了pytorch1.8.1和torchvision0.9.1，模型前向计算速度感觉应该是正常了，但是会有警告输出。此外，我新建了一个虚拟环境（命名为pytorch），在该环境下安装测试了pytorch-1.9.0 （linux_aarch64那个，manylinux2014_aarch64安装后使用异常） + torchvision-0.10.0 + torchaudio-0.9.0（推荐安装这个版本），安装后用使用正常，没有奇怪的警告输出，且使用YOLO_v5进行测试也正常。（1.10.0版本安装后存在问题，其他版本未提供whl文件或缺少torchvision的whl文件）***3.2.2 从源码进行编译安装$\\color{red}{NOTE: 以下部分为非必要操作}$NOTE：在Raspberry PI OS aarch64系统下测试可行，未在Ubuntu Mate上测试，在Ubuntu Mate上应该是一致的。自己手动编译安装Pytorch、torchvision、torchaudio（未测试）此处我编译安装的版本为：pytorch 1.8.1，torchvisioin 0.9.1（参考官方github readme，以及用树莓派4b构建深度学习应用（四）PyTorch篇和树莓派安装pytorch，史上最全方法合集（附安装链接））&amp;gt;1. 依赖库的安装及一些预操作# 在虚拟环境 raspi 下进行编译和安装conda activate raspi# 依赖库的安装来源于官方github readme，# 其中mkl mkl-include库的安装有问题，无法找到相关包conda install astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclassesconda install pillow# 临时编译参数设置，可设置的参数很多，此处参考的是其他blog的设置，# 具体有哪些可设置的参数也没找到一个详细的介绍（先这样吧）export NO_CUDA=1export NO_DISTRIBUTED=1export NO_MKLDNN=1export NO_NNPACK=1export NO_QNNPACK=1&amp;gt;2. 下载pytorch源码并拉取所要安装的分支版本cd ~/Downloadsgit clone --recursive https://github.com/pytorch/pytorchcd pytorchgit checkout v1.8.1git submodule update --init --recursive&amp;gt;3. 编译pytorch生成whl文件并安装python setup.py bdist_wheel# 生成的 whl 文件存储于 dist 路径下cd dist# 安装 whlpip install torchvision-0.9.0a0+8fb5838-cp37-cp37m-linux_aarch64.whl&amp;gt;4. 下载torchvision源码并拉取与pytorch版本匹配的分支版本cd ~/Downloadsgit clone https://github.com/pytorch/visioncd vision# pytorch 1.8.1 对应 vision 版本为 0.9.1git checkout v0.9.1git submodule update --init --recursive&amp;gt;5. 编译torchvision生成whl文件并安装python setup.py bdist_wheelcd distpip install torchvision-0.9.0a0+8fb5838-cp37-cp37m-linux_aarch64.whl&amp;gt;6. 测试测试结果正常，前向推理速度应该也正常。yolo_v5测试结果也正常" }, { "title": "PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录", "url": "/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/", "categories": "Blogs, Pytorch", "tags": "pytorch", "date": "2021-12-23 00:00:00 +0800", "snippet": "不做具体的原理分析和介绍（因为我也不咋懂），针对我实际修改可用的一个用法介绍，主要是模型训练入口主函数（main_multi_gpu.py）的四处修改。以上的介绍来源https://zhuanlan.zhihu.com/p/2064678520. 概述使用DDP进行单机多卡训练时，通过多进程在多个GPU上复制模型，每个GPU都由一个进程控制，同时需要将参数local_rank传递给进程，用于表示当前进程使用的是哪一个GPU。要将单机单卡训练修改为基于DDP的单机多卡训练，需要进行的修改如下（总共四处需要修改）： 初始化设置，需要设置local_rank参数，并需要将local_rank参数传递到进程中，如下： # 在参数设置中添加local_rank参数，见parse_args()函数 # 运行时无需指定local_rank参数，但必须在此处进行定义 parser.add_argument(&quot;--local_rank&quot;, type=int, default=0) # 将local_rank参数传入到进程中，见init_distributed_mode()函数 # local_rank：表示当前GPU编号 # local_world_size：使用的GPU数量 torch.cuda.set_device(local_rank) torch.distributed.init_process_group( backend=&#39;nccl&#39;, init_method=&#39;env://&#39;, world_size=local_world_size, rank=local_rank ) DataLoader修改，需要使用torch.utils.data.DistributedSampler对数据集进行划分（把所有数据划分给不同的GPU），用于多个GPU单独的数据读取：此处，Dataset –&amp;gt; DistributedSampler –&amp;gt; BatchSampler –&amp;gt; DataLoader此外，Dataset、Sampler、DataLoader的关系可参考博文一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系，我觉得写的还挺清楚的（大概可以总结为，Dataset包含了所有的数据，Sampler指明了需要用到的数据的index，DataLoader根据Sampler从Dataset中提取数据用于训练） # 见setup_loader()函数中修改 &quot;&quot;&quot; # 从数据集中采样划分出每块GPU所用的数据，组织形式为数据的index，如下： 32104 99491 11488 25070 67216 22453 57418 45591 64625 46036 98404 81477 &quot;&quot;&quot; self.sampler_train = torch.utils.data.DistributedSampler( xx_set ) &quot;&quot;&quot; # 将每个GPU所用的数据，按照BATCH_SIZE进行组织，组织形式为数据index的list，list长度即为BATCH_SIZE，如下： [32104, 99491, 11488, 25070, 67216, 22453, 57418, 45591, 64625, 46036] [98404, 81477, 73638, 22696, 82657, 44563, 106537, 15772, 85536, 38823] ...... &quot;&quot;&quot; batch_sampler_train = torch.utils.data.BatchSampler( self.sampler_train, batch_size=cfg.TRAIN.BATCH_SIZE, drop_last=cfg.DATA_LOADER.DROP_LAST ) # 根据batch_sampler_train（数据index）从xx_set数据集中提取出真实数据用于训练 self.training_loader = torch.utils.data.DataLoader( self.xx_set, # 数据集 batch_sampler=batch_sampler_train, # 该GPU分配到的数据（以batch为单位组织） collate_fn=datasets.data_loader.sample_collate, # 一个batch数据的组织方式 pin_memory = cfg.DATA_LOADER.PIN_MEMORY, num_workers=cfg.DATA_LOADER.NUM_WORKERS ) 模型初始化设置，使用torch.nn.parallel.DistributedDataParallel对模型进行包装，需要传入local_rank参数 # 使用torch.nn.parallel.DistributedDataParallel对模型进行包装，同时传入local_rank参数，表明模型是在哪一块GPU上 self.model = torch.nn.parallel.DistributedDataParallel( model.to(self.device), device_ids=[self.local_rank] # 传入`local_rank`参数，即GPU编号 ) 模型训练代码修改，训练过程中每开始新的epoch，基于torch.utils.data.BatchSampler创建的self.sampler_train都需要调用set_epoch(epoch_num)对训练集数据进行shuffle；并且需要调用torch.distributed.barrier()。见train()函数中修改内容如果不使用BatchSampler而直接使用Dataset –&amp;gt; DistributedSampler –&amp;gt; DataLoader（需要在DataLoader中指定batch_size参数），应该不用调用set_epoch(epoch_num)？（不确定，没试过）。 1. 单机多卡训练代码修改模型训练入口主函数文件组织大致如下：# main_multi_gpu.py# 导入相关包import torchimport ...# 训练器定义class Trainer(object): def __init__(self, args): super(Trainer, self).__init__() self.args = args # 固定随机数种子 ... # 判断是否为多卡训练 self.num_gpus = torch.cuda.device_count() self.distributed = self.num_gpus &amp;gt; 1 # 针对单机多卡训练，进行初始化 # 单机单卡训练无需此操作 if self.distributed: # 获取该进程的local_rank，数值和args.local_rank一致 self.local_rank = init_distributed_mode() self.device = torch.device(&quot;cuda&quot;) if self.num_gpus &amp;gt; 0 else torch.device(&quot;cpu&quot;) # 训练集构建 self.setup_dataset() self.setup_loader(0) # 0表示初始epoch # 模型结构构建 self.setup_network() # 一些无关的初始化操作 ... def setup_dataset(self): self.xx_set = ... # 创建训练集 def setup_loader(self, epoch): if self.distributed: self.sampler_train = torch.utils.data.DistributedSampler( self.xx_set ) else: self.sampler_train = torch.utils.data.RandomSampler( self.xx_set ) batch_sampler_train = torch.utils.data.BatchSampler( self.sampler_train, batch_size=cfg.TRAIN.BATCH_SIZE, drop_last=cfg.DATA_LOADER.DROP_LAST ) self.training_loader = torch.utils.data.DataLoader( self.xx_set, batch_sampler=batch_sampler_train, collate_fn=datasets.data_loader.sample_collate, # pin_memory = cfg.DATA_LOADER.PIN_MEMORY, num_workers=cfg.DATA_LOADER.NUM_WORKERS ) def setup_netword(self): model = ... # 构建模型结构 # DDP模型 if self.distributed: self.model = torch.nn.parallel.DistributedDataParallel( model.to(self.device), device_ids=[self.local_rank] ) else: self.model = torch.nn.parallel.DataParallel(model).to(self.device) # 一些不太相关的操作，比如损失函数、模型训练优化器的设置 self.xe_criterion = ... # 模型训练交叉熵损失 self.optim = ... # 模型训练优化器 ... # 模型训练核心 def train(): self.model.train() # 训练过程Epoch循环 for epoch in range(MAX_EPOCH): # 如果是单机多卡训练，需要调用set_epoch，将数据进行shuffle if self.distributed: self.sampler_train.set_epoch(epoch) # 每个Epoch训练过程中的iteration循环 for _data_ in self.training_loader: # 1 模型前向运算，并计算损失 loss = ... # 2 梯度清零 self.optim.zero_grad() # 3 计算新梯度，并进行梯度裁减（梯度裁减为可选操作） loss.backward() # 4 梯度反传，模型参数更新 self.optim.step() # 又是一些不太相关的操作，比如优化器lr衰减 ... # 进程间数据同步？（不确定是不是必须操作） if self.distributed: torch.distributed.barrier() # 又是一些不太相关的操作，比如模型的保存，模型的验证 ... # 进程间数据同步？ if self.distributed: torch.distributed.barrier()# 参数def parse_args(): &#39;&#39;&#39; Parse input arguments &#39;&#39;&#39; parser = argparse.ArgumentParser(description=&#39;Image Captioning&#39;) # 模型训练所需一些参数 ... # DDP训练所需参数，--local_rank，不加它也有办法能跑起来，但是还是加了更规范一点 parser.add_argument(&quot;--local_rank&quot;, type=int, default=0) if len(sys.argv) == 1: parser.print_help() sys.exit(1) args = parser.parse_args() return args# 以下两个函数参考了DETR# 禁用非主进程的输出def setup_for_distributed(is_master): &quot;&quot;&quot; This function disables printing when not in master process &quot;&quot;&quot; import builtins as __builtin__ builtin_print = __builtin__.print def print(*args, **kwargs): force = kwargs.pop(&#39;force&#39;, False) if is_master or force: builtin_print(*args, **kwargs) __builtin__.print = printdef init_distributed_mode(): # 获取GPU编号 # 初始化时使用get_rank()报错，难道只能初始化之后才能正常调用获取local_rank值？ # local_rank = torch.distributed.get_rank() # local_world_size = torch.distributed.get_world_size() # 也可以传入args，通过args.local_rank获取local_rank值 （即当前GPU编号） # 通过torch.cuda.device_count()获取local_world_size值 （即GPU数量） if &#39;RANK&#39; in os.environ and &#39;WORLD_SIZE&#39; in os.environ: local_rank = int(os.environ[&quot;RANK&quot;]) local_world_size = int(os.environ[&#39;WORLD_SIZE&#39;]) local_gpu = int(os.environ[&#39;LOCAL_RANK&#39;]) else: print(&#39;Error when get init distributed settings!&#39;) return torch.cuda.set_device(local_rank) print(&#39;| distributed init (rank {}): env://&#39;.format(local_rank), flush=True) torch.distributed.init_process_group( backend=&#39;nccl&#39;, init_method=&#39;env://&#39;, world_size=local_world_size, # 所有的进程数，及GPU数量 rank=local_rank ) torch.distributed.barrier() # 禁用非主进程的输出，local_rank为0的进程作为主进程 setup_for_distributed(local_rank==0) # 返回GPU编号 return local_rankif __name__ == &#39;__main__&#39;: args = parse_args() if args.folder is not None: cfg_from_file(os.path.join(args.folder, &#39;config.yml&#39;)) cfg.ROOT_DIR = args.folder trainer = Trainer(args) trainer.train()单机单卡训练（--*** ***表示模型训练所需传入的其他参数）CUDA_VISIBLE_DEVICES=0 python main_multi_gpu.py --*** ***单机多卡训练，无需指定--local_rank参数，使用torch.distributed.launch可以自动指定相关参数，在代码中可以从os.environ中获取相关参数（见init_distributed_mode()函数），但是必须得设置parser.add_argument(&quot;--local_rank&quot;, type=int, default=0)CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --master_port=3141 --nproc_per_node 2 main_multi_gpu.py --*** ***参考：[1] 一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系[2] DataParallel &amp;amp; DistributedDataParallel分布式训练[3] [原创][深度][PyTorch] DDP系列第一篇：入门教程[4] DETR源码 https://github.com/facebookresearch/detr" }, { "title": "MacOS下iterm，Dracula主题配置", "url": "/posts/MacOS%E4%B8%8Biterm_Dracula%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/", "categories": "Blogs, 环境配置", "tags": "环境配置", "date": "2020-11-17 00:00:00 +0800", "snippet": "前提：已安装Git和Anaconda环境 Git：应该是安装Command_Line_Tools_for_Xcode之后即可 Anaconda：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/，从清华镜像站下载安装即可 安装brew：参考 mac安装homebrew失败怎么办？ - 金牛肖马的回答 - 知乎 /bin/zsh -c &quot;$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot; 安装wget： brew install wget 1 安装iterm2前往iterm2官网下载安装2 安装oh-my-zshGitHub仓库地址：https://github.com/ohmyzsh/ohmyzsh安装方法如下（貌似需要科学skr上网）：# curl下载sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;# wget下载sh -c &quot;$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;# fetch下载sh -c &quot;$(fetch -o - https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;配置文件的备份与创建# 备份原配置文件cp ~/.zshrc ~/.zshrc_bp# 创建新的配置文件cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrcNote: 如果在这之前安装了Anaconda，可能需要将原配置文件中的环境变量设置拷贝到新的配置文件中。3 安装PowerLinepip install powerline-statusNote：此处我已经安装过Anaconda4 安装PowerFontsGitHub仓库：https://github.com/powerline/fonts好像很多主题必须得改用Meslo字体，否则会导致显示乱码。# 新建文件夹，用来存储相关资源mkdir -p ~/The/Path/U/Likecd ~/The/Path/U/Like# 下载源码git clone https://github.com/powerline/fonts.git --depth=1cd fonts# 安装字体./install.sh安装好字体之后，进入iterm2的Preferences中进行设置5 安装Dracula主题5.1 Dracula for ZSHGitHub仓库地址：https://github.com/dracula/zsh# 新建文件夹，用来存储Dracula相关资源mkdir -p ~/The/Path/U/Like/Draculacd ~/The/Path/U/Like/Dracula# 下载主题资源git clone https://github.com/dracula/zsh.git# 复制文件cp ./zsh/dracula.zsh-theme ~/.oh-my-zsh/themes/# 参考GitHub仓库issue#11，https://github.com/dracula/zsh/issues/11cd ./zsh/cp -r ./lib/ ~/.oh-my-zsh/themes/安装完成之后，需要修改~/.zshrc文件，参见https://draculatheme.com/zsh，如下：修改之后，激活~/.zshrc文件source ~/.zshrc5.2 Dracula for iterm2GitHub仓库地址：https://github.com/dracula/itermcd ~/The/Path/U/Like/Dracula# 下载主题资源git clone https://github.com/dracula/iterm.git设置主题，参见https://draculatheme.com/iterm：设置完成之后，如下：5.3 Dracula for vimGitHub仓库：https://github.com/dracula/vim安装参见https://draculatheme.com/vimmkdir -p ~/.vim/pack/themes/startcd ~/.vim/pack/themes/startgit clone https://github.com/dracula/vim draculavi ~/.vimrc6 安装高亮和命令补全插件高亮插件，GitHub仓库地址：https://github.com/zsh-users/zsh-syntax-highlighting命令补全插件，GitHub仓库地址：https://github.com/zsh-users/zsh-autosuggestionscd ~/.oh-my-zsh/custom/pluginsgit clone https://github.com/zsh-users/zsh-syntax-highlightinggit clone https://github.com/zsh-users/zsh-autosuggestions# 修改~/.zshrc文件vi ~/.zshrc# 修改之后激活环境source ~/.zshrc修改内容如下：plugins=( git zsh-autosuggestions zsh-syntax-highlighting)# 并在文件末尾添加source ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh参考博客： https://zhuanlan.zhihu.com/p/37195261 https://blog.csdn.net/daiyuhe/article/details/88667875" }, { "title": "Ubuntu 16.04编译配置opencv 4.1.1 + opencv_contrib 4.1.1（C++ &amp; Python）", "url": "/posts/Ubuntu-16.04%E7%BC%96%E8%AF%91%E9%85%8D%E7%BD%AEopencv-4.1.1-+-opencv_contrib-4.1.1(C++-&-Python)/", "categories": "Blogs, 环境配置", "tags": "环境配置", "date": "2020-07-23 00:00:00 +0800", "snippet": "0 引言主要参考资料： Comprehensive guide to installing OpenCV 4.1.0 on Ubuntu 18.04 from sourceUbuntu 16.04上安装了Anaconda 3（python 3.7.4）的环境，装有opencv-python (4.2.0)包，发现一直没有配置OpenCV（C++）环境。目的也就是编译安装OpenCV（C++）环境，为了测试ViBe官方开源C代码在Ubuntu上的效率，从而试图找出【利用Cython打包复用ViBe运动目标检测C源码】一开始python调用速度不是很快的原因。OpenCV官网也上不去，加载半天没反应，百度各种配置过程教程，结果总是卡在编译make过程，看make报错信息，大概知道是由Anaconda环境导致的错误，里面有一小节报错信息是这样：/usr/bin/ld: warning: libzstd.so.1.3.7, needed by //home/***/anaconda3/lib/libtiff.so.5, not found (try using -rpath or -rpath-link)猜测Anaconda的libtiff.so.5和系统本身下载（见参考的博客的Step 1: Installing dependencies of OpenCV，预先下载依赖项中包含libtiff）的存在冲突？从而就有如下两个方案： 将.bashrc中anaconda坏境变量注释，重新source .bashrc，试了不行，仍然报错； 直接conda deactivate退出当前anaconda环境（默认的应该是base），然后竟然编译安装成功了。（打码的地方是用户名）1 编译安装过程1.1 安装依赖库这一部分，完全按着参考博客【Comprehensive guide to installing OpenCV 4.1.0 on Ubuntu 18.04 from source】中的Step 1来进行的，照着安装即可，毕竟很多东西其实我也不知道是什么。由于我一直使用的是Anaconda的python环境，所以参考博客中Step 2 到 Step5，我都没有用到。1.2 下载OpenCV及OpenCV_Contrib源码我下载的是OpenCV 4.1.1和OpenCV Contrib 4.1.1的源码（版本号一致），下载完成之后将其解压wget https://github.com/opencv/opencv/archive/4.1.1.zipwget https://github.com/opencv/opencv_contrib/archive/4.1.1.zip1.3 构建、编译OpenCVconda deactivate # 我编译成功的关键，退出当前anaconda虚拟环境cd opencv-4.1.1mkdir build cd build构建过程应该算是核心操作了吧，具体参数的意义参考【Comprehensive guide to installing OpenCV 4.1.0 on Ubuntu 18.04 from source】中Step 7介绍，我使用的构建参数如下：cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local/opencv-4.1.1 \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D WITH_TBB=OFF \\ -D WITH_V4L=ON \\ -D WITH_QT=OFF \\ -D WITH_OPENGL=OFF \\ -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-4.1.1/modules \\ -D BUILD_EXAMPLES=ON \\ -D OPENCV_GENERATE_PKGCONFIG=YES .. # 注意最后的两个点儿其中-D CMAKE_INSTALL_PREFIX=/usr/local/opencv-4.1.1用于指定编译安装位置，不指定的话默认应该是装在/use/local；-D INSTALL_C_EXAMPLES=ON和-D INSTALL_PYTHON_EXAMPLES=ON应该是可要可不要，接下来的四个-D WITH_***看个人需要ON或者OFF吧；-D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-4.1.1/modules指定OpenCV Contrib的路径，如果不需要删掉这一行就行，也不用下载它的源码；-D OPENCV_GENERATE_PKGCONFIG=YES用于生成opencv4.pc文件，方便今后编译代码使用。编译过程，可以先使用nproc命令查找可以并行运行的线程数，然后使用make -jn来进行编译，n最好小于nproc查询出的值。nprocmake -j20 # 根据个人机器修改然后就是等，如果运气好的话，就100%编译成功了，如果遇上什么奇奇怪怪的问题，那就再去查吧。我的机器到这是成功编译了的，我觉得我编译成功的关键就是那句conda deactivate。1.4 安装以及环境配置编译成功之后，进行安装，需要root权限：sudo make install安装完之后，其路径位于/usr/local/opencv-4.1.1，需要添加动态库路径，相当于告诉系统下次编译C++代码时，动态链接库到哪里去找。sudo sh -c &#39;echo &quot;/usr/local/opencv-4.1.1/lib&quot; &amp;gt;&amp;gt; /etc/ld.so.conf.d/opencv.conf&#39;sudo ldconfig1.5 opencv4.pc文件的修改与重定位构建的时候说过-D OPENCV_GENERATE_PKGCONFIG=YES用于生成opencv4.pc文件，方便今后编译代码使用。我们可以在路径/usr/local/opencv-4.1.1/lib/pkgconfig/下找到该文件，将其复制到/usr/local/pkgconfig/路径下，如果不存在就创建这个目录：cd /usr/local/libmkdir pkgconfigsudo cp /usr/local/opencv-4.1.1/lib/pkgconfig/opencv4.pc /usr/local/lib/pkgconfig/然后对该文件进行修改，修改参考的【Comprehensive guide to installing OpenCV 4.1.0 on Ubuntu 18.04 from source】中Step 9介绍，我没有测试不改行不行，反正改了是可行的。cd /usr/local/lib/pkgconfigsudo vi opencv4.pc将includedir_old=${prefix}/include/opencv4/opencv改为includedir_old=${prefix}/include/opencv4/opencv2这一些工作完成后，我们就可以进行测试了2 测试命令行输入：pkg-config --libs --cflags opencv4输出-I/usr/local/opencv-4.1.1/include/opencv4/opencv2 -I/usr/local/opencv-4.1.1/include/opencv4 -L/usr/local/opencv-4.1.1/lib -lopencv_stitching -lopencv_gapi -lopencv_freetype -lopencv_img_hash -lopencv_bioinspired -lopencv_stereo -lopencv_tracking -lopencv_face -lopencv_xobjdetect -lopencv_rgbd -lopencv_fuzzy -lopencv_xphoto -lopencv_dnn_objdetect -lopencv_surface_matching -lopencv_sfm -lopencv_saliency -lopencv_plot -lopencv_structured_light -lopencv_xfeatures2d -lopencv_shape -lopencv_phase_unwrapping -lopencv_videostab -lopencv_photo -lopencv_reg -lopencv_hdf -lopencv_hfs -lopencv_aruco -lopencv_datasets -lopencv_text -lopencv_dnn -lopencv_bgsegm -lopencv_ccalib -lopencv_line_descriptor -lopencv_dpm -lopencv_objdetect -lopencv_highgui -lopencv_quality -lopencv_ml -lopencv_superres -lopencv_optflow -lopencv_ximgproc -lopencv_video -lopencv_calib3d -lopencv_videoio -lopencv_imgcodecs -lopencv_features2d -lopencv_imgproc -lopencv_flann -lopencv_core输出来自上面opencv4.pc文件中的内容，实际上也就是指定了编译OpenCV的C++代码时，编译器需要的与OpenCV相关的库文件。即起到了gcc / g++编译时，-I、-L和-l的作用。（相当于【Code::Blocks16.01配置MinGW64及opencv4.1.1】时在Code::Blocks中配置OpenCV的操作）2.1 C++代码的测试新建文件OpenCV_Demo.cpp：#include &quot;opencv.hpp&quot;using namespace cv;using namespace std;int main( int argc, char** argv ){ cout &amp;lt;&amp;lt; &quot;OpenCV version : &quot; &amp;lt;&amp;lt; CV_VERSION &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;Major version : &quot; &amp;lt;&amp;lt; CV_MAJOR_VERSION &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;Minor version : &quot; &amp;lt;&amp;lt; CV_MINOR_VERSION &amp;lt;&amp;lt; endl; cout &amp;lt;&amp;lt; &quot;Subminor version : &quot; &amp;lt;&amp;lt; CV_SUBMINOR_VERSION &amp;lt;&amp;lt; endl;}编译：g++ -std=c++11 main.cpp `pkg-config --libs --cflags opencv4` -o main运行：./main2.2 Python代码测试事实上，我这安装的只会对C++环境有影响，Python环境下的OpenCV仍然是我之前通过anaconda安装的opencv-python，但是编译OpenCV源码过程中也会生成可供Python使用的动态链接库，我们可以将Anaconda里面的opencv-python动态链接库文件替换为新生成的动态链接库文件。Anaconda中opencv-python安装位置如下，里面包含动态库文件cv2.cpython-37m-x86_64-linux-gnu.so：YOUR_ANACONDA_PATH/anaconda3/lib/python3.7/site-packages/cv2我们编译OpenCV源码新生成的动态链接库文件位置如下，里面包含动态链接库文件python-3.5/cv2.cpython-35m-x86_64-linux-gnu.so:/usr/local/opencv-4.1.1/lib/python3.5/dist-packages/cv2如果需要使用我们编译的动态链接库作为python版本的opencv，可以替换掉Anaconda里的动态链接库（不建议直接替换，最好留个备份，因为我也不知道会不会有什么隐患，我看了下我编译生成的动态库文件只有7M多，但是Anaconda下载的有27M，不清楚会不会有什么问题）：cd YOUR_ANACONDA_PATH/anaconda3/lib/python3.7/site-packages/cv2mv cv2.cpython-37m-x86_64-linux-gnu.so old_cv2.cpython-37m-x86_64-linux-gnu.socp /usr/local/opencv-4.1.1/lib/python3.5/dist-packages/cv2/python-3.5/cv2.cpython-35m-x86_64-linux-gnu.so ./cv2.cpython-37m-x86_64-linux-gnu.so然后就可以测试了，测试之前需要将我们的Anaconda虚拟环境再次激活：conda activate base可以看到opencv的版本从本来的4.2.0变成了编译的4.1.1。" } ]
