<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录" /><meta name="author" content="curya" /><meta property="og:locale" content="en" /><meta name="description" content="不做具体的原理分析和介绍（因为我也不咋懂），针对我实际修改可用的一个用法介绍，主要是模型训练入口主函数（main_multi_gpu.py）的四处修改。 以上的介绍来源https://zhuanlan.zhihu.com/p/206467852" /><meta property="og:description" content="不做具体的原理分析和介绍（因为我也不咋懂），针对我实际修改可用的一个用法介绍，主要是模型训练入口主函数（main_multi_gpu.py）的四处修改。 以上的介绍来源https://zhuanlan.zhihu.com/p/206467852" /><link rel="canonical" href="https://232525.github.io/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/" /><meta property="og:url" content="https://232525.github.io/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/" /><meta property="og:site_name" content="Yiyu Wang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-23T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录" /><meta name="twitter:site" content="@ricardo232525" /><meta name="twitter:creator" content="@curya" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"curya"},"dateModified":"2022-04-06T18:59:02+08:00","datePublished":"2021-12-23T00:00:00+08:00","description":"不做具体的原理分析和介绍（因为我也不咋懂），针对我实际修改可用的一个用法介绍，主要是模型训练入口主函数（main_multi_gpu.py）的四处修改。 以上的介绍来源https://zhuanlan.zhihu.com/p/206467852","headline":"PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录","mainEntityOfPage":{"@type":"WebPage","@id":"https://232525.github.io/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/"},"url":"https://232525.github.io/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/"}</script><title>PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录 | Yiyu Wang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Yiyu Wang"><meta name="application-name" content="Yiyu Wang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/favicons/android-chrome-512x512.png " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Yiyu Wang</a></div><div class="site-subtitle font-italic">没人枪毙你，你就活着！</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/about_cn/" class="nav-link"> <i class="fa-fw fa fa-user-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>个人简介</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/232525" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['wangyiyu18','mails.ucas.ac.cn'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://blog.csdn.net/ricardo232525" aria-label="csdn" target="_blank" rel="noopener"> <i class="fas fa-link"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/232525">Yiyu Wang</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1640188800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2021-12-23 </em> </span> <span> Updated <em class="timeago" data-ts="1649242742" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-04-06 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2008 words"> <em>11 min</em> read</span></div></div></div><div class="post-content"><p>不做具体的原理分析和介绍（因为我也不咋懂），针对我实际修改可用的一个用法介绍，主要是模型训练入口主函数（main_multi_gpu.py）的四处修改。 <img data-src="https://img-blog.csdnimg.cn/b4875bde1e7d45e5b6443b4104b59b70.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ3VyeWE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" data-proofer-ignore> 以上的介绍来源<a href="https://zhuanlan.zhihu.com/p/206467852">https://zhuanlan.zhihu.com/p/206467852</a></p><h2 id="0-概述"><span class="mr-2">0. 概述</span><a href="#0-概述" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>使用DDP进行单机多卡训练时，通过多进程在多个GPU上复制模型，每个GPU都由一个进程控制，同时需要将参数<code class="language-plaintext highlighter-rouge">local_rank</code>传递给进程，用于表示当前进程使用的是哪一个GPU。</p><p>要将单机单卡训练修改为基于DDP的单机多卡训练，需要进行的修改如下（总共四处需要修改）：</p><ol><li><p>初始化设置，需要设置<code class="language-plaintext highlighter-rouge">local_rank</code>参数，并需要将<code class="language-plaintext highlighter-rouge">local_rank</code>参数传递到进程中，如下：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre> <span class="c1"># 在参数设置中添加local_rank参数，见parse_args()函数
</span> <span class="c1"># 运行时无需指定local_rank参数，但必须在此处进行定义
</span> <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--local_rank"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

 <span class="c1"># 将local_rank参数传入到进程中，见init_distributed_mode()函数
</span> <span class="c1"># local_rank：表示当前GPU编号
</span> <span class="c1"># local_world_size：使用的GPU数量
</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span>
     <span class="n">backend</span><span class="o">=</span><span class="s">'nccl'</span><span class="p">,</span>
     <span class="n">init_method</span><span class="o">=</span><span class="s">'env://'</span><span class="p">,</span>
     <span class="n">world_size</span><span class="o">=</span><span class="n">local_world_size</span><span class="p">,</span>
     <span class="n">rank</span><span class="o">=</span><span class="n">local_rank</span>
 <span class="p">)</span>
</pre></table></code></div></div><li><p>DataLoader修改，需要使用<code class="language-plaintext highlighter-rouge">torch.utils.data.DistributedSampler</code>对数据集进行划分（把所有数据划分给不同的GPU），用于多个GPU单独的数据读取： 此处，Dataset –&gt; DistributedSampler –&gt; BatchSampler –&gt; DataLoader 此外，Dataset、Sampler、DataLoader的关系可参考博文<a href="https://www.cnblogs.com/marsggbo/p/11308889.html">一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系</a>，我觉得写的还挺清楚的（<strong>大概可以总结为，Dataset包含了所有的数据，Sampler指明了需要用到的数据的index，DataLoader根据Sampler从Dataset中提取数据用于训练</strong>）</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre><td class="rouge-code"><pre> <span class="c1"># 见setup_loader()函数中修改
</span> <span class="s">"""
 # 从数据集中采样划分出每块GPU所用的数据，组织形式为数据的index，如下：
     32104
     99491
     11488
     25070
     67216
     22453
     57418
     45591
     64625
     46036
     98404
     81477
 """</span>
 <span class="bp">self</span><span class="p">.</span><span class="n">sampler_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DistributedSampler</span><span class="p">(</span>
     <span class="n">xx_set</span>
 <span class="p">)</span>
 <span class="s">"""
 # 将每个GPU所用的数据，按照BATCH_SIZE进行组织，组织形式为数据index的list，list长度即为BATCH_SIZE，如下：
     [32104, 99491, 11488, 25070, 67216, 22453, 57418, 45591, 64625, 46036]
     [98404, 81477, 73638, 22696, 82657, 44563, 106537, 15772, 85536, 38823]
     ......
 """</span>
 <span class="n">batch_sampler_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">BatchSampler</span><span class="p">(</span>
     <span class="bp">self</span><span class="p">.</span><span class="n">sampler_train</span><span class="p">,</span>
     <span class="n">batch_size</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">TRAIN</span><span class="p">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
     <span class="n">drop_last</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">DATA_LOADER</span><span class="p">.</span><span class="n">DROP_LAST</span>
 <span class="p">)</span>
 <span class="c1"># 根据batch_sampler_train（数据index）从xx_set数据集中提取出真实数据用于训练
</span> <span class="bp">self</span><span class="p">.</span><span class="n">training_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
     <span class="bp">self</span><span class="p">.</span><span class="n">xx_set</span><span class="p">,</span> <span class="c1"># 数据集
</span>     <span class="n">batch_sampler</span><span class="o">=</span><span class="n">batch_sampler_train</span><span class="p">,</span> <span class="c1"># 该GPU分配到的数据（以batch为单位组织）
</span>     <span class="n">collate_fn</span><span class="o">=</span><span class="n">datasets</span><span class="p">.</span><span class="n">data_loader</span><span class="p">.</span><span class="n">sample_collate</span><span class="p">,</span>  <span class="c1"># 一个batch数据的组织方式
</span>     <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">DATA_LOADER</span><span class="p">.</span><span class="n">PIN_MEMORY</span><span class="p">,</span>
     <span class="n">num_workers</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">DATA_LOADER</span><span class="p">.</span><span class="n">NUM_WORKERS</span>
 <span class="p">)</span>
</pre></table></code></div></div><li><p>模型初始化设置，使用<code class="language-plaintext highlighter-rouge">torch.nn.parallel.DistributedDataParallel</code>对模型进行包装，需要传入<code class="language-plaintext highlighter-rouge">local_rank</code>参数</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre> <span class="c1"># 使用torch.nn.parallel.DistributedDataParallel对模型进行包装，同时传入local_rank参数，表明模型是在哪一块GPU上
</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
     <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">),</span>
     <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">local_rank</span><span class="p">]</span>  <span class="c1"># 传入`local_rank`参数，即GPU编号
</span> <span class="p">)</span>
</pre></table></code></div></div><li><p>模型训练代码修改，训练过程中每开始新的epoch，基于<code class="language-plaintext highlighter-rouge">torch.utils.data.BatchSampler</code>创建的<code class="language-plaintext highlighter-rouge">self.sampler_train</code>都需要调用<code class="language-plaintext highlighter-rouge">set_epoch(epoch_num)</code>对训练集数据进行shuffle；并且需要调用<code class="language-plaintext highlighter-rouge">torch.distributed.barrier()</code>。见<code class="language-plaintext highlighter-rouge">train()</code>函数中修改内容 如果不使用<code class="language-plaintext highlighter-rouge">BatchSampler</code>而直接使用Dataset –&gt; DistributedSampler –&gt; DataLoader（需要在DataLoader中指定batch_size参数），应该不用调用<code class="language-plaintext highlighter-rouge">set_epoch(epoch_num)</code>？（不确定，没试过）。</p></ol><h2 id="1-单机多卡训练代码修改"><span class="mr-2">1. 单机多卡训练代码修改</span><a href="#1-单机多卡训练代码修改" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>模型训练入口主函数文件组织大致如下：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
</pre><td class="rouge-code"><pre><span class="c1"># main_multi_gpu.py
# 导入相关包
</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">...</span>

<span class="c1"># 训练器定义
</span><span class="k">class</span> <span class="nc">Trainer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Trainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="c1"># 固定随机数种子
</span>        <span class="p">...</span>

		<span class="c1"># 判断是否为多卡训练
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">num_gpus</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">distributed</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span>

        <span class="c1"># 针对单机多卡训练，进行初始化
</span>        <span class="c1"># 单机单卡训练无需此操作
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">distributed</span><span class="p">:</span>
        	<span class="c1"># 获取该进程的local_rank，数值和args.local_rank一致
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="n">init_distributed_mode</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>

        <span class="c1"># 训练集构建
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">setup_dataset</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">setup_loader</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 0表示初始epoch
</span>        <span class="c1"># 模型结构构建
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">setup_network</span><span class="p">()</span>
        <span class="c1"># 一些无关的初始化操作
</span>        <span class="p">...</span>

	<span class="k">def</span> <span class="nf">setup_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">xx_set</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># 创建训练集
</span>
	<span class="k">def</span> <span class="nf">setup_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
		<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">sampler_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DistributedSampler</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">xx_set</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">sampler_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">RandomSampler</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">xx_set</span>
            <span class="p">)</span>
        <span class="n">batch_sampler_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">BatchSampler</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">sampler_train</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">TRAIN</span><span class="p">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">DATA_LOADER</span><span class="p">.</span><span class="n">DROP_LAST</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">training_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">xx_set</span><span class="p">,</span>
            <span class="n">batch_sampler</span><span class="o">=</span><span class="n">batch_sampler_train</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="n">datasets</span><span class="p">.</span><span class="n">data_loader</span><span class="p">.</span><span class="n">sample_collate</span><span class="p">,</span>  <span class="c1">#
</span>            <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">DATA_LOADER</span><span class="p">.</span><span class="n">PIN_MEMORY</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">DATA_LOADER</span><span class="p">.</span><span class="n">NUM_WORKERS</span>
        <span class="p">)</span>

	<span class="k">def</span> <span class="nf">setup_netword</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">model</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># 构建模型结构
</span>		<span class="c1"># DDP模型
</span>		<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
                <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">),</span>
                <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">local_rank</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

		<span class="c1"># 一些不太相关的操作，比如损失函数、模型训练优化器的设置
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">xe_criterion</span> <span class="o">=</span> <span class="p">...</span> <span class="c1"># 模型训练交叉熵损失
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">optim</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># 模型训练优化器
</span>        <span class="p">...</span>

	<span class="c1"># 模型训练核心
</span>	<span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
		<span class="c1"># 训练过程Epoch循环
</span>		<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_EPOCH</span><span class="p">):</span>
			<span class="c1"># 如果是单机多卡训练，需要调用set_epoch，将数据进行shuffle
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">distributed</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">sampler_train</span><span class="p">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="c1"># 每个Epoch训练过程中的iteration循环
</span>            <span class="k">for</span> <span class="n">_data_</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">training_loader</span><span class="p">:</span>
            	<span class="c1"># 1 模型前向运算，并计算损失
</span>            	<span class="n">loss</span> <span class="o">=</span> <span class="p">...</span>
            	<span class="c1"># 2 梯度清零
</span>            	<span class="bp">self</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            	<span class="c1"># 3 计算新梯度，并进行梯度裁减（梯度裁减为可选操作）
</span>            	<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            	<span class="c1"># 4 梯度反传，模型参数更新
</span>            	<span class="bp">self</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

				<span class="c1"># 又是一些不太相关的操作，比如优化器lr衰减
</span>				<span class="p">...</span>

				<span class="c1"># 进程间数据同步？（不确定是不是必须操作）
</span>				<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">distributed</span><span class="p">:</span>
                	<span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">barrier</span><span class="p">()</span>
        	<span class="c1"># 又是一些不太相关的操作，比如模型的保存，模型的验证
</span>        	<span class="p">...</span>
        	<span class="c1"># 进程间数据同步？
</span>			<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">distributed</span><span class="p">:</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">barrier</span><span class="p">()</span>

<span class="c1"># 参数
</span><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
    <span class="s">'''
    Parse input arguments
    '''</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">'Image Captioning'</span><span class="p">)</span>
    <span class="c1"># 模型训练所需一些参数
</span>    <span class="p">...</span>
    <span class="c1"># DDP训练所需参数，--local_rank，不加它也有办法能跑起来，但是还是加了更规范一点
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--local_rank"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">parser</span><span class="p">.</span><span class="n">print_help</span><span class="p">()</span>
        <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">args</span>

<span class="c1"># 以下两个函数参考了DETR
# 禁用非主进程的输出
</span><span class="k">def</span> <span class="nf">setup_for_distributed</span><span class="p">(</span><span class="n">is_master</span><span class="p">):</span>
    <span class="s">"""
    This function disables printing when not in master process
    """</span>
    <span class="kn">import</span> <span class="nn">builtins</span> <span class="k">as</span> <span class="n">__builtin__</span>
    <span class="n">builtin_print</span> <span class="o">=</span> <span class="n">__builtin__</span><span class="p">.</span><span class="k">print</span>

    <span class="k">def</span> <span class="nf">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">force</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'force'</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_master</span> <span class="ow">or</span> <span class="n">force</span><span class="p">:</span>
            <span class="n">builtin_print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">__builtin__</span><span class="p">.</span><span class="k">print</span> <span class="o">=</span> <span class="k">print</span>

<span class="k">def</span> <span class="nf">init_distributed_mode</span><span class="p">():</span>
    <span class="c1"># 获取GPU编号
</span>    <span class="c1"># 初始化时使用get_rank()报错，难道只能初始化之后才能正常调用获取local_rank值？
</span>    <span class="c1"># local_rank = torch.distributed.get_rank()
</span>    <span class="c1"># local_world_size = torch.distributed.get_world_size()
</span>
    <span class="c1"># 也可以传入args，通过args.local_rank获取local_rank值 （即当前GPU编号）
</span>    <span class="c1"># 通过torch.cuda.device_count()获取local_world_size值 （即GPU数量）
</span>    <span class="k">if</span> <span class="s">'RANK'</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span> <span class="ow">and</span> <span class="s">'WORLD_SIZE'</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"RANK"</span><span class="p">])</span>
        <span class="n">local_world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'WORLD_SIZE'</span><span class="p">])</span>
        <span class="n">local_gpu</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'LOCAL_RANK'</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Error when get init distributed settings!'</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'| distributed init (rank {}): env://'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">local_rank</span><span class="p">),</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span>
        <span class="n">backend</span><span class="o">=</span><span class="s">'nccl'</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="s">'env://'</span><span class="p">,</span>
        <span class="n">world_size</span><span class="o">=</span><span class="n">local_world_size</span><span class="p">,</span> <span class="c1"># 所有的进程数，及GPU数量
</span>        <span class="n">rank</span><span class="o">=</span><span class="n">local_rank</span>
    <span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">barrier</span><span class="p">()</span>
    <span class="c1"># 禁用非主进程的输出，local_rank为0的进程作为主进程
</span>    <span class="n">setup_for_distributed</span><span class="p">(</span><span class="n">local_rank</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># 返回GPU编号
</span>    <span class="k">return</span> <span class="n">local_rank</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">folder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cfg_from_file</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">folder</span><span class="p">,</span> <span class="s">'config.yml'</span><span class="p">))</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">ROOT_DIR</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">folder</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
</pre></table></code></div></div><p>单机单卡训练（<code class="language-plaintext highlighter-rouge">--*** ***</code>表示模型训练所需传入的其他参数）</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>0 python main_multi_gpu.py <span class="nt">--</span><span class="k">***</span> <span class="k">***</span>
</pre></table></code></div></div><p>单机多卡训练，无需指定<code class="language-plaintext highlighter-rouge">--local_rank</code>参数，使用<code class="language-plaintext highlighter-rouge">torch.distributed.launch</code>可以自动指定相关参数，在代码中可以从<code class="language-plaintext highlighter-rouge">os.environ</code>中获取相关参数（见<code class="language-plaintext highlighter-rouge">init_distributed_mode()</code>函数），但是必须得设置<code class="language-plaintext highlighter-rouge">parser.add_argument("--local_rank", type=int, default=0)</code></p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>0,1 python <span class="nt">-m</span> torch.distributed.launch <span class="nt">--master_port</span><span class="o">=</span>3141 <span class="nt">--nproc_per_node</span> 2 main_multi_gpu.py <span class="nt">--</span><span class="k">***</span> <span class="k">***</span>
</pre></table></code></div></div><h2 id="参考"><span class="mr-2">参考：</span><a href="#参考" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p><a href="https://www.cnblogs.com/marsggbo/p/11308889.html">[1] 一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系</a> <a href="https://zhuanlan.zhihu.com/p/206467852">[2] DataParallel &amp; DistributedDataParallel分布式训练</a> <a href="https://zhuanlan.zhihu.com/p/178402798">[3] [原创][深度][PyTorch] DDP系列第一篇：入门教程</a> <a href="https://github.com/facebookresearch/detr">[4] DETR源码 https://github.com/facebookresearch/detr</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blogs/'>Blogs</a>, <a href='/categories/pytorch/'>Pytorch</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >pytorch</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录 - Yiyu Wang&amp;url=https://232525.github.io/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录 - Yiyu Wang&amp;u=https://232525.github.io/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://232525.github.io/posts/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95/&amp;text=PyTorch单机多卡训练（DDP-DistributedDataParallel的使用）备忘记录 - Yiyu Wang" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Qwen-VL/">Qwen-VL</a><li><a href="/posts/DeepSeek-OCR/">DeepSeek-OCR论文阅读记录</a><li><a href="/posts/BBPE/">BBPE笔记记录</a><li><a href="/posts/RoPE-YARN/">RoPE + YARN</a><li><a href="/posts/%E5%9B%9B%E7%A7%8DNormalization%E7%9A%84%E8%AE%A1%E7%AE%97%E5%B7%AE%E5%BC%82/">四种Normalization的计算差异</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/basicknow/">BasicKnow</a> <a class="post-tag" href="/tags/leetcode/">leetcode</a> <a class="post-tag" href="/tags/touchfish/">touchfish</a> <a class="post-tag" href="/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">环境配置</a> <a class="post-tag" href="/tags/daily/">Daily</a> <a class="post-tag" href="/tags/normalization/">Normalization</a> <a class="post-tag" href="/tags/pytorch/">pytorch</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/%E5%9B%9B%E7%A7%8DNormalization%E7%9A%84%E8%AE%A1%E7%AE%97%E5%B7%AE%E5%BC%82/"><div class="card-body"> <em class="timeago small" data-ts="1657179600" > 2022-07-07 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>四种Normalization的计算差异</h3><div class="text-muted small"><p> Refs: 深度学习中的Normalization方法 将输入维度记为$[N, C, H, W]$，在计算操作上，不同Normalization的主要区别在于： Batch Normalization：在Batch Size方向上，对NHW做归一化，对batch size大小比较敏感； Layer Normalization：在Channel方向上，对CHW归一化； Inst...</p></div></div></a></div><div class="card"> <a href="/posts/RoPE-YARN/"><div class="card-body"> <em class="timeago small" data-ts="1755619200" > 2025-08-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RoPE + YARN</h3><div class="text-muted small"><p> 1. RoPE（旋转位置编码） 将二维向量 $(x_1, x_2)$ 绕原点旋转 $\theta$ 角度，用矩阵乘法表示如下： \[\begin{pmatrix} x_1&#39; \\ x_2&#39; \end{pmatrix} = \begin{pmatrix} \cos\theta &amp;amp; -\sin\theta \\ \sin\theta &amp;amp; \cos\theta \end{p...</p></div></div></a></div><div class="card"> <a href="/posts/Ubuntu-16.04%E7%BC%96%E8%AF%91%E9%85%8D%E7%BD%AEopencv-4.1.1-+-opencv_contrib-4.1.1(C++-&-Python)/"><div class="card-body"> <em class="timeago small" data-ts="1595433600" > 2020-07-23 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Ubuntu 16.04编译配置opencv 4.1.1 + opencv_contrib 4.1.1（C++ & Python）</h3><div class="text-muted small"><p> 0 引言 主要参考资料： Comprehensive guide to installing OpenCV 4.1.0 on Ubuntu 18.04 from source Ubuntu 16.04上安装了Anaconda 3（python 3.7.4）的环境，装有opencv-python (4.2.0)包，发现一直没有配置OpenCV（C++）环境。目的也就是编译安装OpenCV（C...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/MacOS%E4%B8%8Biterm_Dracula%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/" class="btn btn-outline-primary" prompt="Older"><p>MacOS下iterm，Dracula主题配置</p></a> <a href="/posts/%E6%A0%91%E8%8E%93%E6%B4%BE4B_8G%E5%86%85%E5%AD%98%E7%89%88%E6%9C%AC_%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEUbuntu_Mate_20.04_arm64_%E8%AE%B0%E5%BD%95/" class="btn btn-outline-primary" prompt="Newer"><p>树莓派4B（8G内存版本）安装配置Ubuntu Mate 20.04（arm64）记录</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "232525/232525.github.io", "data-repo-id": "R_kgDOHDi0nw", "data-category": "Announcements", "data-category-id": "DIC_kwDOHDi0n84COcbz", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/232525">Yiyu Wang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/basicknow/">BasicKnow</a> <a class="post-tag" href="/tags/leetcode/">leetcode</a> <a class="post-tag" href="/tags/touchfish/">touchfish</a> <a class="post-tag" href="/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">环境配置</a> <a class="post-tag" href="/tags/daily/">Daily</a> <a class="post-tag" href="/tags/normalization/">Normalization</a> <a class="post-tag" href="/tags/pytorch/">pytorch</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
